{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "gpus = [2]\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=','.join([str(i) for i in gpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow.keras as keras\n",
    "import numpy as np \n",
    "#from oe_acute import MNE\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "#from AE import MDSAE as ae\n",
    "#from network_visualisation import plot_these_aud_weights\n",
    "#import network_visualisation\n",
    "#import quantify_aud_strfs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocate GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = [2] # Here I set CUDA to only see one GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=','.join([str(i) for i in gpus])\n",
    "num_gpus = len(gpus) # number of GPUs to use\n",
    "if len(gpus) < 1:\n",
    "    num_gpus = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print( [x.name for x in local_device_protos if x.device_type == 'GPU'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_Specgram_CNN_Model(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Temporal_Specgram_CNN_Model, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.enc_1=keras.Sequential([\n",
    "            keras.Input(shape=(16, 16, 1)),\n",
    "#             keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\",),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size=3, strides=(1, 1), activation=\"relu\",),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            \n",
    "            \n",
    "        ])\n",
    "        self.enc_dropout=tf.keras.layers.Dropout(0.5)\n",
    "        self.enc_2=tf.keras.layers.Dense(units=256,activation='sigmoid', kernel_regularizer=keras.regularizers.L1(10**-3.5),)\n",
    "        self.dec_recon=keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=8 * 8 * 256, activation=\"relu\", kernel_regularizer=keras.regularizers.L1(10**-3.5)),\n",
    "            tf.keras.layers.Reshape(target_shape=(8, 8, 256)),\n",
    "#             tf.keras.layers.Conv2DTranspose(\n",
    "#                 filters=16, kernel_size=2, strides=(2, 2),  activation=\"relu\", kernel_regularizer=keras.regularizers.L1(10**-3.5), \n",
    "#             ),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=8, kernel_size=2, strides=(2, 2), activation=\"relu\", kernel_regularizer=keras.regularizers.L1(10**-3.5)\n",
    "            ),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=1, kernel_size=1, strides=(1, 1), kernel_regularizer=keras.regularizers.L1(10**-3.5)\n",
    "            ),\n",
    "        ])\n",
    "        self.dec_pred=keras.Sequential([tf.keras.layers.Dense(units=16, kernel_regularizer=keras.regularizers.L1(10**-3.5))])\n",
    "        self.recon_losses=[]\n",
    "        self.pred_losses=[]\n",
    "    @tf.function\n",
    "    def get_loss(self, x_t, y_t):\n",
    "        #print(x_t)#.shape\n",
    "        x_hat, y_hat = self(tf.expand_dims(x_t, -1))\n",
    "        pred_losses=tf.reduce_mean(tf.square(y_t - y_hat))\n",
    "        \n",
    "        recon_losses=tf.reduce_mean(tf.square(x_t - tf.squeeze(x_hat, -1)))\n",
    "\n",
    "        \n",
    "        return pred_losses,recon_losses\n",
    "\n",
    "    @tf.function\n",
    "    def get_gradients(self, x_t, y_t):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_losses,recon_losses = self.get_loss(x_t, y_t)\n",
    "            #tf.print(pred_losses, recon_losses)\n",
    "            loss=pred_losses+0.5*recon_losses\n",
    "            \n",
    "        return loss, tape.gradient(loss, self.enc_1.trainable_variables+self.enc_2.trainable_variables+self.dec_recon.trainable_variables+self.dec_pred.trainable_variables)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_model(self, X_train, y_train):\n",
    "        loss, gradients = self.get_gradients(X_train, y_train)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.enc_1.trainable_variables+self.enc_2.trainable_variables+self.dec_recon.trainable_variables+self.dec_pred.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def compute_test_loss(self, X_test, y_test):\n",
    "        x_hat, y_hat = self.predict(tf.expand_dims(X_test, -1))\n",
    "        pred_losses=tf.reduce_mean(tf.square(y_test - y_hat))\n",
    "        \n",
    "        recon_losses=tf.reduce_mean(tf.square(X_test - tf.squeeze(x_hat, -1)))\n",
    "\n",
    "        \n",
    "        return pred_losses,recon_losses\n",
    "\n",
    "\n",
    "    def call(self, input):\n",
    "        latent=self.enc_1(input)\n",
    "        latent=self.enc_dropout(latent, training=True)\n",
    "        latent=self.enc_2(latent)\n",
    "        return self.dec_recon(latent), self.dec_pred(latent)\n",
    "\n",
    "    def predict(self, input):\n",
    "        latent=self.enc_1(input)\n",
    "        latent=self.enc_dropout(latent, training=False)\n",
    "        latent=self.enc_2(latent)\n",
    "        return self.dec_recon(latent), self.dec_pred(latent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.enc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_t, y_t = train_batch[0], train_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_losses,recon_losses=model.get_loss(x_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def extract_spec_data(x_path, y_path, random_seed=None, global_normalize=False):\n",
    "def extract_spec_data(x_path, y_path, random_seed=None):\n",
    "    #all_curr=np.load(x_path,allow_pickle=True)\n",
    "    #all_next=np.load(y_path,allow_pickle=True)\n",
    "    all_curr = pickle.load(open(x_path, 'rb'))\n",
    "    all_next = pickle.load(open(y_path, 'rb'))\n",
    "    \n",
    "    x_array = all_curr\n",
    "    y_array = all_next\n",
    "\n",
    "    #x_array = np.vstack(all_curr)\n",
    "    #y_array = np.vstack(all_next)\n",
    "\n",
    "    if random_seed is None:\n",
    "        rand_idx=np.arange(0, np.shape(x_array)[0])\n",
    "    else:\n",
    "        np.random.seed(random_seed)\n",
    "        rand_idx=np.random.choice(range(np.shape(x_array)[0]), size=np.shape(x_array)[0],replace=False)\n",
    "    \n",
    "    split_train_idx, split_val_idx = rand_idx[np.shape(x_array)[0]//10:],rand_idx[:np.shape(x_array)[0]//10] \n",
    "    x_train, x_val=np.asarray(x_array)[split_train_idx], np.asarray(x_array)[split_val_idx]\n",
    "    y_train, y_val=np.asarray(y_array)[split_train_idx],np.asarray(y_array)[split_val_idx]\n",
    "    \n",
    "    #if global_normalize:\n",
    "    #    x_train=x_train/x_train.max()\n",
    "    #    y_train=y_train/y_train.max()\n",
    "    #    x_val=x_val/x_val.max()\n",
    "    #    y_val=y_val/y_val.max()\n",
    "    return x_train,y_train,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs_path = '/mnt/cube/srrudrar/stim_preprocess/temporal_model/segs_list_16.pkl'\n",
    "next_path = '/mnt/cube/srrudrar/stim_preprocess/temporal_model/next_list_16.pkl'\n",
    "\n",
    "#x_train,y_train,x_val,y_val = extract_spec_data(segs_path, next_path, random_seed=0,global_normalize=True)\n",
    "x_train1,y_train1,x_val1,y_val1 = extract_spec_data(segs_path, next_path, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train1[:, :16, :]\n",
    "x_val = x_val1[:, :16, :]\n",
    "y_train = y_train1[:, :16]\n",
    "y_val = y_val1[:, :16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train-Loss 0.02586805634200573\n",
      "Epoch 1, Train-Loss 0.042931362986564636\n",
      "Epoch 2, Train-Loss 0.024631988257169724\n",
      "Epoch 3, Train-Loss 0.02788478694856167\n",
      "Epoch 4, Train-Loss 0.022729631513357162\n",
      "Epoch 5, Train-Loss 0.024003690108656883\n",
      "Epoch 6, Train-Loss 0.01789757050573826\n",
      "Epoch 7, Train-Loss 0.019721107557415962\n",
      "Epoch 8, Train-Loss 0.021156208589673042\n",
      "Epoch 9, Train-Loss 0.015912465751171112\n",
      "Epoch 10, Train-Loss 0.007782127242535353\n",
      "Epoch 11, Train-Loss 0.018873313441872597\n",
      "Epoch 12, Train-Loss 0.015400153584778309\n",
      "Epoch 13, Train-Loss 0.01788967102766037\n",
      "Epoch 14, Train-Loss 0.01412546169012785\n",
      "Epoch 15, Train-Loss 0.014795362949371338\n",
      "Epoch 16, Train-Loss 0.017832648009061813\n",
      "Epoch 17, Train-Loss 0.015187372453510761\n",
      "Epoch 18, Train-Loss 0.01711355894804001\n",
      "Epoch 19, Train-Loss 0.010669895447790623\n",
      "Epoch 20, Train-Loss 0.01222964283078909\n",
      "Epoch 21, Train-Loss 0.0109123345464468\n",
      "Epoch 22, Train-Loss 0.012260456569492817\n",
      "Epoch 23, Train-Loss 0.012336455285549164\n",
      "Epoch 24, Train-Loss 0.012570499442517757\n",
      "Epoch 25, Train-Loss 0.011658803559839725\n",
      "Epoch 26, Train-Loss 0.01024060882627964\n",
      "Epoch 27, Train-Loss 0.015862885862588882\n",
      "Epoch 28, Train-Loss 0.013590920716524124\n",
      "Epoch 29, Train-Loss 0.012525690719485283\n",
      "Epoch 30, Train-Loss 0.008456820622086525\n",
      "Epoch 31, Train-Loss 0.01271851733326912\n",
      "Epoch 32, Train-Loss 0.010571926832199097\n",
      "Epoch 33, Train-Loss 0.011913272552192211\n",
      "Epoch 34, Train-Loss 0.014180246740579605\n",
      "Epoch 35, Train-Loss 0.009174050763249397\n",
      "Epoch 36, Train-Loss 0.010399671271443367\n",
      "Epoch 37, Train-Loss 0.014989354647696018\n",
      "Epoch 38, Train-Loss 0.013454969972372055\n",
      "Epoch 39, Train-Loss 0.01029464416205883\n",
      "Epoch 40, Train-Loss 0.009705726057291031\n",
      "Epoch 41, Train-Loss 0.010180842131376266\n",
      "Epoch 42, Train-Loss 0.008899742737412453\n",
      "Epoch 43, Train-Loss 0.009858052246272564\n",
      "Epoch 44, Train-Loss 0.007571672089397907\n",
      "Epoch 45, Train-Loss 0.010145648382604122\n",
      "Epoch 46, Train-Loss 0.010222358629107475\n",
      "Epoch 47, Train-Loss 0.006666381843388081\n",
      "Epoch 48, Train-Loss 0.009797964245080948\n",
      "Epoch 49, Train-Loss 0.00938008725643158\n",
      "Epoch 50, Train-Loss 0.009838099591434002\n",
      "Epoch 51, Train-Loss 0.014393921941518784\n",
      "Epoch 52, Train-Loss 0.011318838223814964\n",
      "Epoch 53, Train-Loss 0.010758972726762295\n",
      "Epoch 54, Train-Loss 0.007680858485400677\n",
      "Epoch 55, Train-Loss 0.01384385209530592\n",
      "Epoch 56, Train-Loss 0.007506540976464748\n",
      "Epoch 57, Train-Loss 0.006525530479848385\n",
      "Epoch 58, Train-Loss 0.009315194562077522\n",
      "Epoch 59, Train-Loss 0.008996816352009773\n",
      "Epoch 60, Train-Loss 0.012168696150183678\n",
      "Epoch 61, Train-Loss 0.007392451167106628\n",
      "Epoch 62, Train-Loss 0.007305188104510307\n",
      "Epoch 63, Train-Loss 0.007005752995610237\n",
      "Epoch 64, Train-Loss 0.007533214055001736\n",
      "Epoch 65, Train-Loss 0.009581075049936771\n",
      "Epoch 66, Train-Loss 0.008855180814862251\n",
      "Epoch 67, Train-Loss 0.00783394742757082\n",
      "Epoch 68, Train-Loss 0.010197905823588371\n",
      "Epoch 69, Train-Loss 0.009313290938735008\n",
      "Epoch 70, Train-Loss 0.007463955320417881\n",
      "Epoch 71, Train-Loss 0.005568858236074448\n",
      "Epoch 72, Train-Loss 0.011404886841773987\n",
      "Epoch 73, Train-Loss 0.0066483099944889545\n",
      "Epoch 74, Train-Loss 0.00707902479916811\n",
      "Epoch 75, Train-Loss 0.00735888909548521\n",
      "Epoch 76, Train-Loss 0.008816997520625591\n",
      "Epoch 77, Train-Loss 0.009446951560676098\n",
      "Epoch 78, Train-Loss 0.009614893235266209\n",
      "Epoch 79, Train-Loss 0.007960282266139984\n",
      "Epoch 80, Train-Loss 0.008601710200309753\n",
      "Epoch 81, Train-Loss 0.0075542074628174305\n",
      "Epoch 82, Train-Loss 0.007246606983244419\n",
      "Epoch 83, Train-Loss 0.0074125025421381\n",
      "Epoch 84, Train-Loss 0.00873580016195774\n",
      "Epoch 85, Train-Loss 0.0060994685627520084\n",
      "Epoch 86, Train-Loss 0.0075992802157998085\n",
      "Epoch 87, Train-Loss 0.006657310761511326\n",
      "Epoch 88, Train-Loss 0.00837948638945818\n",
      "Epoch 89, Train-Loss 0.00859446544200182\n",
      "Epoch 90, Train-Loss 0.007526332512497902\n",
      "Epoch 91, Train-Loss 0.007252201437950134\n",
      "Epoch 92, Train-Loss 0.008800706826150417\n",
      "Epoch 93, Train-Loss 0.00870706420391798\n",
      "Epoch 94, Train-Loss 0.006771543063223362\n",
      "Epoch 95, Train-Loss 0.010301156900823116\n",
      "Epoch 96, Train-Loss 0.006659260485321283\n",
      "Epoch 97, Train-Loss 0.00618530809879303\n",
      "Epoch 98, Train-Loss 0.010092401877045631\n",
      "Epoch 99, Train-Loss 0.00537127535790205\n",
      "Epoch 100, Train-Loss 0.0070407516323029995\n",
      "Epoch 101, Train-Loss 0.010321740992367268\n",
      "Epoch 102, Train-Loss 0.007464941591024399\n",
      "Epoch 103, Train-Loss 0.006584980525076389\n",
      "Epoch 104, Train-Loss 0.008263567462563515\n",
      "Epoch 105, Train-Loss 0.006469263695180416\n",
      "Epoch 106, Train-Loss 0.008318879641592503\n",
      "Epoch 107, Train-Loss 0.005588860716670752\n",
      "Epoch 108, Train-Loss 0.007455780636519194\n",
      "Epoch 109, Train-Loss 0.005759545601904392\n",
      "Epoch 110, Train-Loss 0.0060798367485404015\n",
      "Epoch 111, Train-Loss 0.007773687597364187\n",
      "Epoch 112, Train-Loss 0.007276951801031828\n",
      "Epoch 113, Train-Loss 0.007710673846304417\n",
      "Epoch 114, Train-Loss 0.006686410866677761\n",
      "Epoch 115, Train-Loss 0.008062070235610008\n",
      "Epoch 116, Train-Loss 0.009311062283813953\n",
      "Epoch 117, Train-Loss 0.008471335284411907\n",
      "Epoch 118, Train-Loss 0.008183073252439499\n",
      "Epoch 119, Train-Loss 0.006866644602268934\n",
      "Epoch 120, Train-Loss 0.006331654265522957\n",
      "Epoch 121, Train-Loss 0.0068601323291659355\n",
      "Epoch 122, Train-Loss 0.007913466542959213\n",
      "Epoch 123, Train-Loss 0.007832664065063\n",
      "Epoch 124, Train-Loss 0.009372404776513577\n",
      "Epoch 125, Train-Loss 0.009204089641571045\n",
      "Epoch 126, Train-Loss 0.007742886897176504\n",
      "Epoch 127, Train-Loss 0.006033800542354584\n",
      "Epoch 128, Train-Loss 0.006923089735209942\n",
      "Epoch 129, Train-Loss 0.006750188302248716\n",
      "Epoch 130, Train-Loss 0.006979774683713913\n",
      "Epoch 131, Train-Loss 0.010179453529417515\n",
      "Epoch 132, Train-Loss 0.005599289201200008\n",
      "Epoch 133, Train-Loss 0.008165731094777584\n",
      "Epoch 134, Train-Loss 0.006376239005476236\n",
      "Epoch 135, Train-Loss 0.00483072130009532\n",
      "Epoch 136, Train-Loss 0.00546609703451395\n",
      "Epoch 137, Train-Loss 0.006113831419497728\n",
      "Epoch 138, Train-Loss 0.00655699148774147\n",
      "Epoch 139, Train-Loss 0.006038384512066841\n",
      "Epoch 140, Train-Loss 0.0069959815591573715\n",
      "Epoch 141, Train-Loss 0.007492750883102417\n",
      "Epoch 142, Train-Loss 0.006164471618831158\n",
      "Epoch 143, Train-Loss 0.006615226622670889\n",
      "Epoch 144, Train-Loss 0.006187227554619312\n",
      "Epoch 145, Train-Loss 0.006649828515946865\n",
      "Epoch 146, Train-Loss 0.005854266230016947\n",
      "Epoch 147, Train-Loss 0.00769889447838068\n",
      "Epoch 148, Train-Loss 0.0062139593064785\n",
      "Epoch 149, Train-Loss 0.007755956146866083\n",
      "Epoch 150, Train-Loss 0.005986122414469719\n",
      "Epoch 151, Train-Loss 0.0059536295011639595\n",
      "Epoch 152, Train-Loss 0.007065922021865845\n",
      "Epoch 153, Train-Loss 0.008128752000629902\n",
      "Epoch 154, Train-Loss 0.006753313355147839\n",
      "Epoch 155, Train-Loss 0.00640135770663619\n",
      "Epoch 156, Train-Loss 0.006424638442695141\n",
      "Epoch 157, Train-Loss 0.007876981049776077\n",
      "Epoch 158, Train-Loss 0.006179912947118282\n",
      "Epoch 159, Train-Loss 0.007921895012259483\n",
      "Epoch 160, Train-Loss 0.006228294689208269\n",
      "Epoch 161, Train-Loss 0.0068336897529661655\n",
      "Epoch 162, Train-Loss 0.007452496327459812\n",
      "Epoch 163, Train-Loss 0.006445733830332756\n",
      "Epoch 164, Train-Loss 0.010311228223145008\n",
      "Epoch 165, Train-Loss 0.0060996366664767265\n",
      "Epoch 166, Train-Loss 0.007589483633637428\n",
      "Epoch 167, Train-Loss 0.00749570457264781\n",
      "Epoch 168, Train-Loss 0.007378796581178904\n",
      "Epoch 169, Train-Loss 0.0061646755784749985\n",
      "Epoch 170, Train-Loss 0.0071293809451162815\n",
      "Epoch 171, Train-Loss 0.006779660936444998\n",
      "Epoch 172, Train-Loss 0.005458639934659004\n",
      "Epoch 173, Train-Loss 0.0076166559010744095\n",
      "Epoch 174, Train-Loss 0.0073539745062589645\n",
      "Epoch 175, Train-Loss 0.007738241460174322\n",
      "Epoch 176, Train-Loss 0.005891528911888599\n",
      "Epoch 177, Train-Loss 0.005795157980173826\n",
      "Epoch 178, Train-Loss 0.00734063982963562\n",
      "Epoch 179, Train-Loss 0.007055849768221378\n",
      "Epoch 180, Train-Loss 0.007281614001840353\n",
      "Epoch 181, Train-Loss 0.005732137244194746\n",
      "Epoch 182, Train-Loss 0.007358344737440348\n",
      "Epoch 183, Train-Loss 0.0071334391832351685\n",
      "Epoch 184, Train-Loss 0.006387418136000633\n",
      "Epoch 185, Train-Loss 0.006898863241076469\n",
      "Epoch 186, Train-Loss 0.0069059329107403755\n",
      "Epoch 187, Train-Loss 0.005930738989263773\n",
      "Epoch 188, Train-Loss 0.008174561895430088\n",
      "Epoch 189, Train-Loss 0.007273731753230095\n",
      "Epoch 190, Train-Loss 0.005682552698999643\n",
      "Epoch 191, Train-Loss 0.008390509523451328\n",
      "Epoch 192, Train-Loss 0.007151584606617689\n",
      "Epoch 193, Train-Loss 0.00621851347386837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, Train-Loss 0.007204554975032806\n",
      "Epoch 195, Train-Loss 0.004571268800646067\n",
      "Epoch 196, Train-Loss 0.007118544541299343\n",
      "Epoch 197, Train-Loss 0.0059296684339642525\n",
      "Epoch 198, Train-Loss 0.00529691856354475\n",
      "Epoch 199, Train-Loss 0.005827834829688072\n",
      "Epoch 200, Train-Loss 0.005248269997537136\n",
      "Epoch 201, Train-Loss 0.006767389830201864\n",
      "Epoch 202, Train-Loss 0.005965727381408215\n",
      "Epoch 203, Train-Loss 0.005892239045351744\n",
      "Epoch 204, Train-Loss 0.006720329634845257\n",
      "Epoch 205, Train-Loss 0.007087564095854759\n",
      "Epoch 206, Train-Loss 0.004940824583172798\n",
      "Epoch 207, Train-Loss 0.005129029508680105\n",
      "Epoch 208, Train-Loss 0.007872088812291622\n",
      "Epoch 209, Train-Loss 0.0062202224507927895\n",
      "Epoch 210, Train-Loss 0.006882693152874708\n",
      "Epoch 211, Train-Loss 0.009154962375760078\n",
      "Epoch 212, Train-Loss 0.0063447849825024605\n",
      "Epoch 213, Train-Loss 0.007031343877315521\n",
      "Epoch 214, Train-Loss 0.007089040242135525\n",
      "Epoch 215, Train-Loss 0.006511089392006397\n",
      "Epoch 216, Train-Loss 0.0060272663831710815\n",
      "Epoch 217, Train-Loss 0.006693256087601185\n",
      "Epoch 218, Train-Loss 0.006888585165143013\n",
      "Epoch 219, Train-Loss 0.007067953702062368\n",
      "Epoch 220, Train-Loss 0.00641275430098176\n",
      "Epoch 221, Train-Loss 0.008662156760692596\n",
      "Epoch 222, Train-Loss 0.006291838362812996\n",
      "Epoch 223, Train-Loss 0.007676481269299984\n",
      "Epoch 224, Train-Loss 0.0068472581915557384\n",
      "Epoch 225, Train-Loss 0.006473531946539879\n",
      "Epoch 226, Train-Loss 0.007066446356475353\n",
      "Epoch 227, Train-Loss 0.006585096009075642\n",
      "Epoch 228, Train-Loss 0.005586504470556974\n",
      "Epoch 229, Train-Loss 0.005074892193078995\n",
      "Epoch 230, Train-Loss 0.006415738724172115\n",
      "Epoch 231, Train-Loss 0.004774212371557951\n",
      "Epoch 232, Train-Loss 0.006119346711784601\n",
      "Epoch 233, Train-Loss 0.006064476445317268\n",
      "Epoch 234, Train-Loss 0.006674851290881634\n",
      "Epoch 235, Train-Loss 0.005692227743566036\n",
      "Epoch 236, Train-Loss 0.007601134944707155\n",
      "Epoch 237, Train-Loss 0.005211879499256611\n",
      "Epoch 238, Train-Loss 0.006942762061953545\n",
      "Epoch 239, Train-Loss 0.006200289353728294\n",
      "Epoch 240, Train-Loss 0.006561607122421265\n",
      "Epoch 241, Train-Loss 0.006499159149825573\n",
      "Epoch 242, Train-Loss 0.007953056134283543\n",
      "Epoch 243, Train-Loss 0.007361972704529762\n",
      "Epoch 244, Train-Loss 0.00998280756175518\n",
      "Epoch 245, Train-Loss 0.00589325837790966\n",
      "Epoch 246, Train-Loss 0.006980822421610355\n",
      "Epoch 247, Train-Loss 0.005399093963205814\n",
      "Epoch 248, Train-Loss 0.007673857733607292\n",
      "Epoch 249, Train-Loss 0.005070278886705637\n",
      "Epoch 250, Train-Loss 0.0068941907957196236\n",
      "Epoch 251, Train-Loss 0.005988641642034054\n",
      "Epoch 252, Train-Loss 0.005483201704919338\n",
      "Epoch 253, Train-Loss 0.007303435355424881\n",
      "Epoch 254, Train-Loss 0.00664775725454092\n",
      "Epoch 255, Train-Loss 0.0068873390555381775\n",
      "Epoch 256, Train-Loss 0.006528133060783148\n",
      "Epoch 257, Train-Loss 0.007213269360363483\n",
      "Epoch 258, Train-Loss 0.005245944019407034\n",
      "Epoch 259, Train-Loss 0.007556167431175709\n",
      "Epoch 260, Train-Loss 0.005073748994618654\n",
      "Epoch 261, Train-Loss 0.007776525802910328\n",
      "Epoch 262, Train-Loss 0.005780263803899288\n",
      "Epoch 263, Train-Loss 0.006923840846866369\n",
      "Epoch 264, Train-Loss 0.006766056641936302\n",
      "Epoch 265, Train-Loss 0.00443123048171401\n",
      "Epoch 266, Train-Loss 0.0054558757692575455\n",
      "Epoch 267, Train-Loss 0.007274754345417023\n",
      "Epoch 268, Train-Loss 0.007104785181581974\n",
      "Epoch 269, Train-Loss 0.006562070921063423\n",
      "Epoch 270, Train-Loss 0.00651133805513382\n",
      "Epoch 271, Train-Loss 0.005834611132740974\n",
      "Epoch 272, Train-Loss 0.007362032774835825\n",
      "Epoch 273, Train-Loss 0.006146647036075592\n",
      "Epoch 274, Train-Loss 0.0072043403051793575\n",
      "Epoch 275, Train-Loss 0.007467638701200485\n",
      "Epoch 276, Train-Loss 0.007376255467534065\n",
      "Epoch 277, Train-Loss 0.006168457679450512\n",
      "Epoch 278, Train-Loss 0.005246474873274565\n",
      "Epoch 279, Train-Loss 0.0041777705773711205\n",
      "Epoch 280, Train-Loss 0.005367277655750513\n",
      "Epoch 281, Train-Loss 0.005903158336877823\n",
      "Epoch 282, Train-Loss 0.004034097772091627\n",
      "Epoch 283, Train-Loss 0.007108170539140701\n",
      "Epoch 284, Train-Loss 0.006032190285623074\n",
      "Epoch 285, Train-Loss 0.005867213476449251\n",
      "Epoch 286, Train-Loss 0.006313744466751814\n",
      "Epoch 287, Train-Loss 0.005455292761325836\n",
      "Epoch 288, Train-Loss 0.006447553168982267\n",
      "Epoch 289, Train-Loss 0.005686424672603607\n",
      "Epoch 290, Train-Loss 0.0050560105592012405\n",
      "Epoch 291, Train-Loss 0.0065967002883553505\n",
      "Epoch 292, Train-Loss 0.006748710758984089\n",
      "Epoch 293, Train-Loss 0.005147432908415794\n",
      "Epoch 294, Train-Loss 0.006977141369134188\n",
      "Epoch 295, Train-Loss 0.006521281786262989\n",
      "Epoch 296, Train-Loss 0.006728185806423426\n",
      "Epoch 297, Train-Loss 0.0059308018535375595\n",
      "Epoch 298, Train-Loss 0.0067627872340381145\n",
      "Epoch 299, Train-Loss 0.005744024645537138\n",
      "Epoch 300, Train-Loss 0.005462913773953915\n",
      "Epoch 301, Train-Loss 0.0053649283945560455\n",
      "Epoch 302, Train-Loss 0.005854103248566389\n",
      "Epoch 303, Train-Loss 0.005664784926921129\n",
      "Epoch 304, Train-Loss 0.006525465287268162\n",
      "Epoch 305, Train-Loss 0.00675061484798789\n",
      "Epoch 306, Train-Loss 0.004920043051242828\n",
      "Epoch 307, Train-Loss 0.004987671039998531\n",
      "Epoch 308, Train-Loss 0.004878906067460775\n",
      "Epoch 309, Train-Loss 0.008479888550937176\n",
      "Epoch 310, Train-Loss 0.006521076429635286\n",
      "Epoch 311, Train-Loss 0.005688168108463287\n",
      "Epoch 312, Train-Loss 0.006107375491410494\n",
      "Epoch 313, Train-Loss 0.005308335181325674\n",
      "Epoch 314, Train-Loss 0.005561190191656351\n",
      "Epoch 315, Train-Loss 0.0056544579565525055\n",
      "Epoch 316, Train-Loss 0.0057794395834207535\n",
      "Epoch 317, Train-Loss 0.0066499970853328705\n",
      "Epoch 318, Train-Loss 0.006307274103164673\n",
      "Epoch 319, Train-Loss 0.0064033144153654575\n",
      "Epoch 320, Train-Loss 0.0054128896445035934\n",
      "Epoch 321, Train-Loss 0.007142954505980015\n",
      "Epoch 322, Train-Loss 0.0065347119234502316\n",
      "Epoch 323, Train-Loss 0.0058318800292909145\n",
      "Epoch 324, Train-Loss 0.006234144791960716\n",
      "Epoch 325, Train-Loss 0.007593303453177214\n",
      "Epoch 326, Train-Loss 0.00617802981287241\n",
      "Epoch 327, Train-Loss 0.0068394215777516365\n",
      "Epoch 328, Train-Loss 0.005696581676602364\n",
      "Epoch 329, Train-Loss 0.0073241256177425385\n",
      "Epoch 330, Train-Loss 0.006563038099557161\n",
      "Epoch 331, Train-Loss 0.007801111321896315\n",
      "Epoch 332, Train-Loss 0.007131471298635006\n",
      "Epoch 333, Train-Loss 0.006687983870506287\n",
      "Epoch 334, Train-Loss 0.005328374914824963\n",
      "Epoch 335, Train-Loss 0.0066630118526518345\n",
      "Epoch 336, Train-Loss 0.005239743739366531\n",
      "Epoch 337, Train-Loss 0.006667129695415497\n",
      "Epoch 338, Train-Loss 0.005277102813124657\n",
      "Epoch 339, Train-Loss 0.007091839332133532\n",
      "Epoch 340, Train-Loss 0.006268156226724386\n",
      "Epoch 341, Train-Loss 0.006426955573260784\n",
      "Epoch 342, Train-Loss 0.005143842659890652\n",
      "Epoch 343, Train-Loss 0.006172356195747852\n",
      "Epoch 344, Train-Loss 0.005348964594304562\n",
      "Epoch 345, Train-Loss 0.006089166272431612\n",
      "Epoch 346, Train-Loss 0.005708254408091307\n",
      "Epoch 347, Train-Loss 0.006383690983057022\n",
      "Epoch 348, Train-Loss 0.006089338101446629\n",
      "Epoch 349, Train-Loss 0.005482042673975229\n",
      "Epoch 350, Train-Loss 0.0077673327177762985\n",
      "Epoch 351, Train-Loss 0.006462838500738144\n",
      "Epoch 352, Train-Loss 0.00633407523855567\n",
      "Epoch 353, Train-Loss 0.0055986130610108376\n",
      "Epoch 354, Train-Loss 0.007433983497321606\n",
      "Epoch 355, Train-Loss 0.005336734000593424\n",
      "Epoch 356, Train-Loss 0.004962821491062641\n",
      "Epoch 357, Train-Loss 0.00639624148607254\n",
      "Epoch 358, Train-Loss 0.00815171655267477\n",
      "Epoch 359, Train-Loss 0.006480281241238117\n",
      "Epoch 360, Train-Loss 0.00492220651358366\n",
      "Epoch 361, Train-Loss 0.007323687430471182\n",
      "Epoch 362, Train-Loss 0.005576700903475285\n",
      "Epoch 363, Train-Loss 0.00602687057107687\n",
      "Epoch 364, Train-Loss 0.005512555129826069\n",
      "Epoch 365, Train-Loss 0.0059003811329603195\n",
      "Epoch 366, Train-Loss 0.004669164307415485\n",
      "Epoch 367, Train-Loss 0.006123179569840431\n",
      "Epoch 368, Train-Loss 0.0062310644425451756\n",
      "Epoch 369, Train-Loss 0.0073768445290625095\n",
      "Epoch 370, Train-Loss 0.006992881186306477\n",
      "Epoch 371, Train-Loss 0.006284340284764767\n",
      "Epoch 372, Train-Loss 0.006514252163469791\n",
      "Epoch 373, Train-Loss 0.00768317561596632\n",
      "Epoch 374, Train-Loss 0.0061735184863209724\n",
      "Epoch 375, Train-Loss 0.005396401509642601\n",
      "Epoch 376, Train-Loss 0.006200282368808985\n",
      "Epoch 377, Train-Loss 0.0060244337655603886\n",
      "Epoch 378, Train-Loss 0.0075020757503807545\n",
      "Epoch 379, Train-Loss 0.006412581540644169\n",
      "Epoch 380, Train-Loss 0.006761600263416767\n",
      "Epoch 381, Train-Loss 0.005333005916327238\n",
      "Epoch 382, Train-Loss 0.005102749448269606\n",
      "Epoch 383, Train-Loss 0.005460808053612709\n",
      "Epoch 384, Train-Loss 0.005443317350000143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385, Train-Loss 0.006793973036110401\n",
      "Epoch 386, Train-Loss 0.005308971274644136\n",
      "Epoch 387, Train-Loss 0.005738988984376192\n",
      "Epoch 388, Train-Loss 0.005003008060157299\n",
      "Epoch 389, Train-Loss 0.005374605767428875\n",
      "Epoch 390, Train-Loss 0.006053150165826082\n",
      "Epoch 391, Train-Loss 0.007606919854879379\n",
      "Epoch 392, Train-Loss 0.007122277282178402\n",
      "Epoch 393, Train-Loss 0.0051483348943293095\n",
      "Epoch 394, Train-Loss 0.007688920013606548\n",
      "Epoch 395, Train-Loss 0.005704872310161591\n",
      "Epoch 396, Train-Loss 0.006090711802244186\n",
      "Epoch 397, Train-Loss 0.005549689754843712\n",
      "Epoch 398, Train-Loss 0.006829993333667517\n",
      "Epoch 399, Train-Loss 0.005935576744377613\n",
      "Epoch 400, Train-Loss 0.0059487484395504\n",
      "Epoch 401, Train-Loss 0.0054137129336595535\n",
      "Epoch 402, Train-Loss 0.006538406480103731\n",
      "Epoch 403, Train-Loss 0.006720905192196369\n",
      "Epoch 404, Train-Loss 0.005439866334199905\n",
      "Epoch 405, Train-Loss 0.007272377144545317\n",
      "Epoch 406, Train-Loss 0.00555410236120224\n",
      "Epoch 407, Train-Loss 0.004846447147428989\n",
      "Epoch 408, Train-Loss 0.005457218736410141\n",
      "Epoch 409, Train-Loss 0.006027309224009514\n",
      "Epoch 410, Train-Loss 0.006491166073828936\n",
      "Epoch 411, Train-Loss 0.005482765845954418\n",
      "Epoch 412, Train-Loss 0.004681820049881935\n",
      "Epoch 413, Train-Loss 0.0072257667779922485\n",
      "Epoch 414, Train-Loss 0.006331070326268673\n",
      "Epoch 415, Train-Loss 0.005691269412636757\n",
      "Epoch 416, Train-Loss 0.005328038707375526\n",
      "Epoch 417, Train-Loss 0.004991529509425163\n",
      "Epoch 418, Train-Loss 0.006977325305342674\n",
      "Epoch 419, Train-Loss 0.006646984256803989\n",
      "Epoch 420, Train-Loss 0.007358689326792955\n",
      "Epoch 421, Train-Loss 0.006189459469169378\n",
      "Epoch 422, Train-Loss 0.005155963823199272\n",
      "Epoch 423, Train-Loss 0.004627847112715244\n",
      "Epoch 424, Train-Loss 0.006024292204529047\n",
      "Epoch 425, Train-Loss 0.005540176760405302\n",
      "Epoch 426, Train-Loss 0.004494067281484604\n",
      "Epoch 427, Train-Loss 0.006422626320272684\n",
      "Epoch 428, Train-Loss 0.004663474392145872\n",
      "Epoch 429, Train-Loss 0.00528838112950325\n",
      "Epoch 430, Train-Loss 0.005421153735369444\n",
      "Epoch 431, Train-Loss 0.005008082836866379\n",
      "Epoch 432, Train-Loss 0.004857955500483513\n",
      "Epoch 433, Train-Loss 0.0038524242118000984\n",
      "Epoch 434, Train-Loss 0.006284206174314022\n",
      "Epoch 435, Train-Loss 0.006310325115919113\n",
      "Epoch 436, Train-Loss 0.006611902266740799\n",
      "Epoch 437, Train-Loss 0.006372825708240271\n",
      "Epoch 438, Train-Loss 0.006262290291488171\n",
      "Epoch 439, Train-Loss 0.005579650867730379\n",
      "Epoch 440, Train-Loss 0.005517659243196249\n",
      "Epoch 441, Train-Loss 0.005406871438026428\n",
      "Epoch 442, Train-Loss 0.005968405865132809\n",
      "Epoch 443, Train-Loss 0.005846443586051464\n",
      "Epoch 444, Train-Loss 0.006164616905152798\n",
      "Epoch 445, Train-Loss 0.004488612059503794\n",
      "Epoch 446, Train-Loss 0.005654310807585716\n",
      "Epoch 447, Train-Loss 0.006116678472608328\n",
      "Epoch 448, Train-Loss 0.00441240007057786\n",
      "Epoch 449, Train-Loss 0.00735879223793745\n",
      "Epoch 450, Train-Loss 0.005103756673634052\n",
      "Epoch 451, Train-Loss 0.005409638863056898\n",
      "Epoch 452, Train-Loss 0.005346661899238825\n",
      "Epoch 453, Train-Loss 0.006667597219347954\n",
      "Epoch 454, Train-Loss 0.005182939115911722\n",
      "Epoch 455, Train-Loss 0.005835189018398523\n",
      "Epoch 456, Train-Loss 0.005618398077785969\n",
      "Epoch 457, Train-Loss 0.005272453650832176\n",
      "Epoch 458, Train-Loss 0.005936594679951668\n",
      "Epoch 459, Train-Loss 0.007010561414062977\n",
      "Epoch 460, Train-Loss 0.005250478629022837\n",
      "Epoch 461, Train-Loss 0.00517297675833106\n",
      "Epoch 462, Train-Loss 0.0056920298375189304\n",
      "Epoch 463, Train-Loss 0.006096205674111843\n",
      "Epoch 464, Train-Loss 0.00582886952906847\n",
      "Epoch 465, Train-Loss 0.007038043811917305\n",
      "Epoch 466, Train-Loss 0.005978296045213938\n",
      "Epoch 467, Train-Loss 0.006893310695886612\n",
      "Epoch 468, Train-Loss 0.003449810203164816\n",
      "Epoch 469, Train-Loss 0.005593377631157637\n",
      "Epoch 470, Train-Loss 0.005282025318592787\n",
      "Epoch 471, Train-Loss 0.004295892082154751\n",
      "Epoch 472, Train-Loss 0.005782340187579393\n",
      "Epoch 473, Train-Loss 0.00756495539098978\n",
      "Epoch 474, Train-Loss 0.0067976126447319984\n",
      "Epoch 475, Train-Loss 0.0053772046230733395\n",
      "Epoch 476, Train-Loss 0.005764738656580448\n",
      "Epoch 477, Train-Loss 0.0054094926454126835\n",
      "Epoch 478, Train-Loss 0.0054941726848483086\n",
      "Epoch 479, Train-Loss 0.004945718217641115\n",
      "Epoch 480, Train-Loss 0.004696252290159464\n",
      "Epoch 481, Train-Loss 0.004414161201566458\n",
      "Epoch 482, Train-Loss 0.004924077540636063\n",
      "Epoch 483, Train-Loss 0.005314108915627003\n",
      "Epoch 484, Train-Loss 0.00643991120159626\n",
      "Epoch 485, Train-Loss 0.00519397109746933\n",
      "Epoch 486, Train-Loss 0.006570935249328613\n",
      "Epoch 487, Train-Loss 0.00606656214222312\n",
      "Epoch 488, Train-Loss 0.005929046310484409\n",
      "Epoch 489, Train-Loss 0.005318884272128344\n",
      "Epoch 490, Train-Loss 0.004107874818146229\n",
      "Epoch 491, Train-Loss 0.00677769910544157\n",
      "Epoch 492, Train-Loss 0.0056167906150221825\n",
      "Epoch 493, Train-Loss 0.00843325350433588\n",
      "Epoch 494, Train-Loss 0.004979489836841822\n",
      "Epoch 495, Train-Loss 0.007209226489067078\n",
      "Epoch 496, Train-Loss 0.007739540655165911\n",
      "Epoch 497, Train-Loss 0.0061045424081385136\n",
      "Epoch 498, Train-Loss 0.007778573781251907\n",
      "Epoch 499, Train-Loss 0.00564545625820756\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dset = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), y_train.astype(np.float32)))\n",
    "train_dset = train_dset.shuffle(buffer_size=x_train.shape[0]+256).batch(64)\n",
    "test_losses = []\n",
    "\n",
    "optimizer = tf.optimizers.Adam(1e-3)\n",
    "model = Temporal_Specgram_CNN_Model(optimizer=optimizer)\n",
    "x_val, y_val = x_val.astype(np.float32), y_val.astype(np.float32)\n",
    "with tf.device('/device:gpu:2'):\n",
    "#     tf.print('Training Fold {}'.format(index))\n",
    "    #model.load_weights('./temporal_specgram_weights/initial')\n",
    "    for epoch in range(500):\n",
    "        #if epoch>250:\n",
    "        #    model.optimizer.learning_rate=2e-4\n",
    "        for step, train_batch in enumerate(train_dset):\n",
    "            train_loss = model.train_model(train_batch[0], train_batch[1])\n",
    "\n",
    "        tf.print('Epoch {}, Train-Loss {}'.format(epoch, train_loss), output_stream=sys.stdout)\n",
    "        test_losses.append(model.compute_test_loss(x_val, y_val))\n",
    "        \n",
    "    plt.plot(test_losses)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Test Loss')\n",
    "    plt.title('Test Loss by Epoch: {} units'.format(256))\n",
    "    #plt.savefig('mnt/cube/srrudrar/temporal_model/loss_plots/{}units_loss.png'.format(256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([15000, 256])\n",
      "TensorShape([15000, 256])\n",
      "TensorShape([18855, 256])\n",
      "TensorShape([5428, 256])\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:gpu:2'):\n",
    "    #model.save_weights('/mnt/cube/srrudrar/temporal_model/temporal_model_weights/temp_256_l1_3_5_500epoch_64batch.h5')\n",
    "    #predicted = model.full_model(x_val)\n",
    "    enc_train1 = model.enc_2(model.enc_dropout(model.enc_1(x_train[:15000,:,:][:,:,:,np.newaxis])))\n",
    "    enc_train2 = model.enc_2(model.enc_dropout(model.enc_1(x_train[15000:30000,:,:][:,:,:,np.newaxis])))\n",
    "    enc_train3 = model.enc_2(model.enc_dropout(model.enc_1(x_train[30000:,:,:][:,:,:,np.newaxis])))\n",
    "    enc_val = model.enc_2(model.enc_dropout(model.enc_1(x_val[:,:,:,np.newaxis])))\n",
    "    #tf.print(predicted.shape)\n",
    "    tf.print(enc_train1.shape)\n",
    "    tf.print(enc_train2.shape)\n",
    "    tf.print(enc_train3.shape)\n",
    "    tf.print(enc_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump([float(i[0].numpy()) for i in test_losses],open('pred_only_64_channels_dropout_test_loss.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
