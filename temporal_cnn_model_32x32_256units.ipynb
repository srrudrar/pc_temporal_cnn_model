{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "gpus = [0]\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=','.join([str(i) for i in gpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow.keras as keras\n",
    "import numpy as np \n",
    "#from oe_acute import MNE\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "#from AE import MDSAE as ae\n",
    "#from network_visualisation import plot_these_aud_weights\n",
    "#import network_visualisation\n",
    "#import quantify_aud_strfs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird = 'B1240'\n",
    "d = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocate GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = [0] # Here I set CUDA to only see one GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=','.join([str(i) for i in gpus])\n",
    "num_gpus = len(gpus) # number of GPUs to use\n",
    "if len(gpus) < 1:\n",
    "    num_gpus = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print( [x.name for x in local_device_protos if x.device_type == 'GPU'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_Specgram_CNN_Model(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Temporal_Specgram_CNN_Model, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.enc_1=keras.Sequential([\n",
    "            keras.Input(shape=(32, 32, 1)),\n",
    "#             keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\",),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size=3, strides=(1, 1), activation=\"relu\",),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            \n",
    "            \n",
    "        ])\n",
    "        self.enc_dropout=tf.keras.layers.Dropout(0.5)\n",
    "        self.enc_2=tf.keras.layers.Dense(units=d,activation='sigmoid', kernel_regularizer=keras.regularizers.L1(10**-3.5),)\n",
    "        self.dec_recon=keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=16 * 16 * 256, activation=\"relu\", kernel_regularizer=keras.regularizers.L1(10**-3.5)),\n",
    "            tf.keras.layers.Reshape(target_shape=(16, 16, 256)),\n",
    "#             tf.keras.layers.Conv2DTfranspose(\n",
    "#                 filters=16, kernel_size=2, strides=(2, 2),  activation=\"relu\", kernel_regularizer=keras.regularizers.L1(10**-3.5), \n",
    "#             ),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=8, kernel_size=2, strides=(2, 2), activation=\"relu\", kernel_regularizer=keras.regularizers.L1(10**-3.5)\n",
    "            ),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=1, kernel_size=1, strides=(1, 1), kernel_regularizer=keras.regularizers.L1(10**-3.5)\n",
    "            ),\n",
    "        ])\n",
    "        self.dec_pred=keras.Sequential([tf.keras.layers.Dense(units=32, kernel_regularizer=keras.regularizers.L1(10**-3.5))])\n",
    "        self.recon_losses=[]\n",
    "        self.pred_losses=[]\n",
    "    @tf.function\n",
    "    def get_loss(self, x_t, y_t):\n",
    "        #print(x_t)#.shape\n",
    "        x_hat, y_hat = self(tf.expand_dims(x_t, -1))\n",
    "        pred_losses=tf.reduce_mean(tf.square(y_t - y_hat))\n",
    "        \n",
    "        recon_losses=tf.reduce_mean(tf.square(x_t - tf.squeeze(x_hat, -1)))\n",
    "\n",
    "        \n",
    "        return pred_losses,recon_losses\n",
    "\n",
    "    @tf.function\n",
    "    def get_gradients(self, x_t, y_t):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_losses,recon_losses = self.get_loss(x_t, y_t)\n",
    "            #tf.print(pred_losses, recon_losses)\n",
    "            loss=pred_losses+0.5*recon_losses\n",
    "            \n",
    "        return loss, tape.gradient(loss, self.enc_1.trainable_variables+self.enc_2.trainable_variables+self.dec_recon.trainable_variables+self.dec_pred.trainable_variables)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_model(self, X_train, y_train):\n",
    "        loss, gradients = self.get_gradients(X_train, y_train)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.enc_1.trainable_variables+self.enc_2.trainable_variables+self.dec_recon.trainable_variables+self.dec_pred.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def compute_test_loss(self, X_test, y_test):\n",
    "        x_hat, y_hat = self.predict(tf.expand_dims(X_test, -1))\n",
    "        pred_losses=tf.reduce_mean(tf.square(y_test - y_hat))\n",
    "        \n",
    "        recon_losses=tf.reduce_mean(tf.square(X_test - tf.squeeze(x_hat, -1)))\n",
    "\n",
    "        \n",
    "        return pred_losses,recon_losses\n",
    "\n",
    "\n",
    "    def call(self, input):\n",
    "        latent=self.enc_1(input)\n",
    "        latent=self.enc_dropout(latent, training=True)\n",
    "        latent=self.enc_2(latent)\n",
    "        return self.dec_recon(latent), self.dec_pred(latent)\n",
    "\n",
    "    def predict(self, input):\n",
    "        latent=self.enc_1(input)\n",
    "        latent=self.enc_dropout(latent, training=False)\n",
    "        latent=self.enc_2(latent)\n",
    "        return self.dec_recon(latent), self.dec_pred(latent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.enc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_t, y_t = train_batch[0], train_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_losses,recon_losses=model.get_loss(x_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spec_data(x_path, y_path, random_seed=None, global_normalize=False):\n",
    "#def extract_spec_data(x_path1, x_path2, x_path3, x_path4, x_path5, y_path, random_seed=None):\n",
    "#def extract_spec_data(x_path1, x_path2, y_path, random_seed=None):\n",
    "    all_curr=np.load(x_path,allow_pickle=True)\n",
    "    #all_next=np.load(y_path,allow_pickle=True)\n",
    "    #all_curr1 = pickle.load(open(x_path1, 'rb'))\n",
    "    #all_curr2 = pickle.load(open(x_path2, 'rb'))\n",
    "    #all_curr3 = pickle.load(open(x_path3, 'rb'))\n",
    "    #all_curr4 = pickle.load(open(x_path4, 'rb'))\n",
    "    #all_curr5 = pickle.load(open(x_path5, 'rb'))\n",
    "\n",
    "    all_next=np.load(y_path,allow_pickle=True)\n",
    "    \n",
    "    x_array = all_curr\n",
    "    #x_array = np.concatenate((all_curr1, all_curr2, all_curr3, all_curr4, all_curr5), axis=0)\n",
    "    #x_array = np.concatenate((all_curr1, all_curr2), axis=0)\n",
    "    y_array = all_next\n",
    "\n",
    "    #x_array = np.vstack(all_curr)\n",
    "    #y_array = np.vstack(all_next)\n",
    "\n",
    "    if random_seed is None:\n",
    "        rand_idx=np.arange(0, np.shape(x_array)[0])\n",
    "    else:\n",
    "        np.random.seed(random_seed)\n",
    "        rand_idx=np.random.choice(range(np.shape(x_array)[0]), size=np.shape(x_array)[0],replace=False)\n",
    "    \n",
    "    split_train_idx, split_val_idx = rand_idx[np.shape(x_array)[0]//10:],rand_idx[:np.shape(x_array)[0]//10] \n",
    "    x_train, x_val=np.asarray(x_array)[split_train_idx], np.asarray(x_array)[split_val_idx]\n",
    "    y_train, y_val=np.asarray(y_array)[split_train_idx],np.asarray(y_array)[split_val_idx]\n",
    "    \n",
    "    #if global_normalize:\n",
    "    #    x_train=x_train/x_train.max()\n",
    "    #    y_train=y_train/y_train.max()\n",
    "    #    x_val=x_val/x_val.max()\n",
    "    #    y_val=y_val/y_val.max()\n",
    "    return x_train,y_train,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs_path = '/mnt/cube/srrudrar/stim_preprocess/temporal_model/{}/segs_list_32.pkl'.format(bird, bird)     ###change\n",
    "#segs_path2 = '/mnt/cube/srrudrar/stim_preprocess/temporal_model/{}/segs_list_{}_32_part2.pkl'.format(bird, bird)     ###change\n",
    "#segs_path3 = '/mnt/cube/srrudrar/stim_preprocess/temporal_model/{}/segs_list_{}_32_part3.pkl'.format(bird, bird)     ###change\n",
    "#segs_path4 = '/mnt/cube/srrudrar/stim_preprocess/temporal_model/{}/segs_list_{}_32_part4.pkl'.format(bird, bird)     ###change\n",
    "#segs_path5 = '/mnt/cube/srrudrar/stim_preprocess/temporal_model/{}/segs_list_{}_32_part5.pkl'.format(bird, bird)     ###change\n",
    "next_path = '/mnt/cube/srrudrar/stim_preprocess/temporal_model/{}/next_list_32.pkl'.format(bird, bird)            ###change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train,y_train,x_val,y_val = extract_spec_data(segs_path, next_path, random_seed=0,global_normalize=True)\n",
    "x_train1,y_train1,x_val1,y_val1 = extract_spec_data(segs_path,next_path,random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train1[:, :32, :]\n",
    "x_val = x_val1[:, :32, :]\n",
    "y_train = y_train1[:, :32]\n",
    "y_val = y_val1[:, :32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51357, 32, 32), (5706, 32, 32), (51357, 32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train), np.shape(x_val), np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train-Loss 0.040806520730257034\n",
      "Epoch 1, Train-Loss 0.04200191795825958\n",
      "Epoch 2, Train-Loss 0.028570037335157394\n",
      "Epoch 3, Train-Loss 0.020732935518026352\n",
      "Epoch 4, Train-Loss 0.03017290309071541\n",
      "Epoch 5, Train-Loss 0.024003680795431137\n",
      "Epoch 6, Train-Loss 0.019875073805451393\n",
      "Epoch 7, Train-Loss 0.016067752614617348\n",
      "Epoch 8, Train-Loss 0.019748441874980927\n",
      "Epoch 9, Train-Loss 0.011057842522859573\n",
      "Epoch 10, Train-Loss 0.009344208985567093\n",
      "Epoch 11, Train-Loss 0.011613276787102222\n",
      "Epoch 12, Train-Loss 0.009030554443597794\n",
      "Epoch 13, Train-Loss 0.01164373941719532\n",
      "Epoch 14, Train-Loss 0.008892755024135113\n",
      "Epoch 15, Train-Loss 0.010286339558660984\n",
      "Epoch 16, Train-Loss 0.00837254524230957\n",
      "Epoch 17, Train-Loss 0.007879850454628468\n",
      "Epoch 18, Train-Loss 0.006967806722968817\n",
      "Epoch 19, Train-Loss 0.007603387348353863\n",
      "Epoch 20, Train-Loss 0.0067965202033519745\n",
      "Epoch 21, Train-Loss 0.007707543671131134\n",
      "Epoch 22, Train-Loss 0.007261364720761776\n",
      "Epoch 23, Train-Loss 0.007869957946240902\n",
      "Epoch 24, Train-Loss 0.00735091045498848\n",
      "Epoch 25, Train-Loss 0.006638323422521353\n",
      "Epoch 26, Train-Loss 0.0056200106628239155\n",
      "Epoch 27, Train-Loss 0.005853736307471991\n",
      "Epoch 28, Train-Loss 0.007221462205052376\n",
      "Epoch 29, Train-Loss 0.007284052670001984\n",
      "Epoch 30, Train-Loss 0.006754777394235134\n",
      "Epoch 31, Train-Loss 0.005170115269720554\n",
      "Epoch 32, Train-Loss 0.004732063040137291\n",
      "Epoch 33, Train-Loss 0.006883218418806791\n",
      "Epoch 34, Train-Loss 0.005644448101520538\n",
      "Epoch 35, Train-Loss 0.00608307309448719\n",
      "Epoch 36, Train-Loss 0.004691275767982006\n",
      "Epoch 37, Train-Loss 0.004869063384830952\n",
      "Epoch 38, Train-Loss 0.00643650908023119\n",
      "Epoch 39, Train-Loss 0.00528292078524828\n",
      "Epoch 40, Train-Loss 0.0055027371272444725\n",
      "Epoch 41, Train-Loss 0.008346614427864552\n",
      "Epoch 42, Train-Loss 0.004666867200285196\n",
      "Epoch 43, Train-Loss 0.00449305260553956\n",
      "Epoch 44, Train-Loss 0.00516465725377202\n",
      "Epoch 45, Train-Loss 0.005583140999078751\n",
      "Epoch 46, Train-Loss 0.004772895947098732\n",
      "Epoch 47, Train-Loss 0.004582839552313089\n",
      "Epoch 48, Train-Loss 0.004838576540350914\n",
      "Epoch 49, Train-Loss 0.005429548677057028\n",
      "Epoch 50, Train-Loss 0.004503059666603804\n",
      "Epoch 51, Train-Loss 0.005193385295569897\n",
      "Epoch 52, Train-Loss 0.004250778816640377\n",
      "Epoch 53, Train-Loss 0.004789258353412151\n",
      "Epoch 54, Train-Loss 0.00503899808973074\n",
      "Epoch 55, Train-Loss 0.004579624161124229\n",
      "Epoch 56, Train-Loss 0.004688676446676254\n",
      "Epoch 57, Train-Loss 0.004914901219308376\n",
      "Epoch 58, Train-Loss 0.004401959478855133\n",
      "Epoch 59, Train-Loss 0.004775453358888626\n",
      "Epoch 60, Train-Loss 0.004764510318636894\n",
      "Epoch 61, Train-Loss 0.00431278720498085\n",
      "Epoch 62, Train-Loss 0.004794130101799965\n",
      "Epoch 63, Train-Loss 0.004519548267126083\n",
      "Epoch 64, Train-Loss 0.0051592071540653706\n",
      "Epoch 65, Train-Loss 0.003967844881117344\n",
      "Epoch 66, Train-Loss 0.00435349065810442\n",
      "Epoch 67, Train-Loss 0.00400135200470686\n",
      "Epoch 68, Train-Loss 0.004017343744635582\n",
      "Epoch 69, Train-Loss 0.00454180920496583\n",
      "Epoch 70, Train-Loss 0.004656984470784664\n",
      "Epoch 71, Train-Loss 0.0041755568236112595\n",
      "Epoch 72, Train-Loss 0.004225295037031174\n",
      "Epoch 73, Train-Loss 0.004253141116350889\n",
      "Epoch 74, Train-Loss 0.004654367454349995\n",
      "Epoch 75, Train-Loss 0.004595096223056316\n",
      "Epoch 76, Train-Loss 0.0050348686054348946\n",
      "Epoch 77, Train-Loss 0.004936239682137966\n",
      "Epoch 78, Train-Loss 0.004776204004883766\n",
      "Epoch 79, Train-Loss 0.0040968721732497215\n",
      "Epoch 80, Train-Loss 0.00399488490074873\n",
      "Epoch 81, Train-Loss 0.0034669237211346626\n",
      "Epoch 82, Train-Loss 0.003512004856020212\n",
      "Epoch 83, Train-Loss 0.004651366733014584\n",
      "Epoch 84, Train-Loss 0.003992038778960705\n",
      "Epoch 85, Train-Loss 0.00478014163672924\n",
      "Epoch 86, Train-Loss 0.0038026701658964157\n",
      "Epoch 87, Train-Loss 0.004436784889549017\n",
      "Epoch 88, Train-Loss 0.003833702765405178\n",
      "Epoch 89, Train-Loss 0.003649601247161627\n",
      "Epoch 90, Train-Loss 0.004219804424792528\n",
      "Epoch 91, Train-Loss 0.003973468206822872\n",
      "Epoch 92, Train-Loss 0.0046070958487689495\n",
      "Epoch 93, Train-Loss 0.0040359580889344215\n",
      "Epoch 94, Train-Loss 0.0037798029370605946\n",
      "Epoch 95, Train-Loss 0.003459525527432561\n",
      "Epoch 96, Train-Loss 0.0037213792093098164\n",
      "Epoch 97, Train-Loss 0.0038876088801771402\n",
      "Epoch 98, Train-Loss 0.004081216640770435\n",
      "Epoch 99, Train-Loss 0.0034301732666790485\n",
      "Epoch 100, Train-Loss 0.004532633814960718\n",
      "Epoch 101, Train-Loss 0.004236905835568905\n",
      "Epoch 102, Train-Loss 0.004607140086591244\n",
      "Epoch 103, Train-Loss 0.004010252188891172\n",
      "Epoch 104, Train-Loss 0.003992326557636261\n",
      "Epoch 105, Train-Loss 0.003765096189454198\n",
      "Epoch 106, Train-Loss 0.00449759466573596\n",
      "Epoch 107, Train-Loss 0.0038580389227718115\n",
      "Epoch 108, Train-Loss 0.00373764312826097\n",
      "Epoch 109, Train-Loss 0.0036613577976822853\n",
      "Epoch 110, Train-Loss 0.004129891283810139\n",
      "Epoch 111, Train-Loss 0.003868310246616602\n",
      "Epoch 112, Train-Loss 0.004666249733418226\n",
      "Epoch 113, Train-Loss 0.0035713191609829664\n",
      "Epoch 114, Train-Loss 0.003966422751545906\n",
      "Epoch 115, Train-Loss 0.0038728569634258747\n",
      "Epoch 116, Train-Loss 0.003901120275259018\n",
      "Epoch 117, Train-Loss 0.0035248324275016785\n",
      "Epoch 118, Train-Loss 0.0035637510009109974\n",
      "Epoch 119, Train-Loss 0.0034385276958346367\n",
      "Epoch 120, Train-Loss 0.003500619437545538\n",
      "Epoch 121, Train-Loss 0.0033430992625653744\n",
      "Epoch 122, Train-Loss 0.003467635251581669\n",
      "Epoch 123, Train-Loss 0.003703936468809843\n",
      "Epoch 124, Train-Loss 0.0034147128462791443\n",
      "Epoch 125, Train-Loss 0.00411299429833889\n",
      "Epoch 126, Train-Loss 0.003304610028862953\n",
      "Epoch 127, Train-Loss 0.003208835143595934\n",
      "Epoch 128, Train-Loss 0.0032841572538018227\n",
      "Epoch 129, Train-Loss 0.0031203818507492542\n",
      "Epoch 130, Train-Loss 0.0040762759745121\n",
      "Epoch 131, Train-Loss 0.0036660023033618927\n",
      "Epoch 132, Train-Loss 0.0052039120346307755\n",
      "Epoch 133, Train-Loss 0.0034765093587338924\n",
      "Epoch 134, Train-Loss 0.00335124833509326\n",
      "Epoch 135, Train-Loss 0.003659830428659916\n",
      "Epoch 136, Train-Loss 0.003745398251339793\n",
      "Epoch 137, Train-Loss 0.00333208404481411\n",
      "Epoch 138, Train-Loss 0.0036831730976700783\n",
      "Epoch 139, Train-Loss 0.00438913656398654\n",
      "Epoch 140, Train-Loss 0.004218310583382845\n",
      "Epoch 141, Train-Loss 0.0033978642895817757\n",
      "Epoch 142, Train-Loss 0.003462649881839752\n",
      "Epoch 143, Train-Loss 0.004357645753771067\n",
      "Epoch 144, Train-Loss 0.002959270030260086\n",
      "Epoch 145, Train-Loss 0.0035378034226596355\n",
      "Epoch 146, Train-Loss 0.0035573686473071575\n",
      "Epoch 147, Train-Loss 0.0037225391715765\n",
      "Epoch 148, Train-Loss 0.0037470757961273193\n",
      "Epoch 149, Train-Loss 0.003299207426607609\n",
      "Epoch 150, Train-Loss 0.0030884065199643373\n",
      "Epoch 151, Train-Loss 0.0034064436331391335\n",
      "Epoch 152, Train-Loss 0.002607602160423994\n",
      "Epoch 153, Train-Loss 0.0033835507929325104\n",
      "Epoch 154, Train-Loss 0.003167717717587948\n",
      "Epoch 155, Train-Loss 0.0034444606862962246\n",
      "Epoch 156, Train-Loss 0.0034578689374029636\n",
      "Epoch 157, Train-Loss 0.003201942890882492\n",
      "Epoch 158, Train-Loss 0.003216516226530075\n",
      "Epoch 159, Train-Loss 0.0034619662910699844\n",
      "Epoch 160, Train-Loss 0.0034912158735096455\n",
      "Epoch 161, Train-Loss 0.003414152655750513\n",
      "Epoch 162, Train-Loss 0.0035648541525006294\n",
      "Epoch 163, Train-Loss 0.00411967933177948\n",
      "Epoch 164, Train-Loss 0.0036498457193374634\n",
      "Epoch 165, Train-Loss 0.003848049556836486\n",
      "Epoch 166, Train-Loss 0.0036059701815247536\n",
      "Epoch 167, Train-Loss 0.0032058944925665855\n",
      "Epoch 168, Train-Loss 0.0034544814843684435\n",
      "Epoch 169, Train-Loss 0.003326899604871869\n",
      "Epoch 170, Train-Loss 0.0029531652107834816\n",
      "Epoch 171, Train-Loss 0.0031584943644702435\n",
      "Epoch 172, Train-Loss 0.0032829763367772102\n",
      "Epoch 173, Train-Loss 0.003127309028059244\n",
      "Epoch 174, Train-Loss 0.003988984972238541\n",
      "Epoch 175, Train-Loss 0.003081798553466797\n",
      "Epoch 176, Train-Loss 0.003477261634543538\n",
      "Epoch 177, Train-Loss 0.003148481482639909\n",
      "Epoch 178, Train-Loss 0.003871499327942729\n",
      "Epoch 179, Train-Loss 0.0034468669909983873\n",
      "Epoch 180, Train-Loss 0.0032732668332755566\n",
      "Epoch 181, Train-Loss 0.00360103533603251\n",
      "Epoch 182, Train-Loss 0.003917469643056393\n",
      "Epoch 183, Train-Loss 0.0030300854705274105\n",
      "Epoch 184, Train-Loss 0.003401735331863165\n",
      "Epoch 185, Train-Loss 0.0031398055143654346\n",
      "Epoch 186, Train-Loss 0.0028929610271006823\n",
      "Epoch 187, Train-Loss 0.0034945940133184195\n",
      "Epoch 188, Train-Loss 0.003813152899965644\n",
      "Epoch 189, Train-Loss 0.0035656122490763664\n",
      "Epoch 190, Train-Loss 0.003558534663170576\n",
      "Epoch 191, Train-Loss 0.0032961273100227118\n",
      "Epoch 192, Train-Loss 0.0031808605417609215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193, Train-Loss 0.0034488514065742493\n",
      "Epoch 194, Train-Loss 0.0029762848280370235\n",
      "Epoch 195, Train-Loss 0.0028624506667256355\n",
      "Epoch 196, Train-Loss 0.002692459849640727\n",
      "Epoch 197, Train-Loss 0.003600427880883217\n",
      "Epoch 198, Train-Loss 0.0028406628407537937\n",
      "Epoch 199, Train-Loss 0.003253828501328826\n",
      "Epoch 200, Train-Loss 0.0033348039723932743\n",
      "Epoch 201, Train-Loss 0.0028639063239097595\n",
      "Epoch 202, Train-Loss 0.0035345484502613544\n",
      "Epoch 203, Train-Loss 0.003287028055638075\n",
      "Epoch 204, Train-Loss 0.0031976266764104366\n",
      "Epoch 205, Train-Loss 0.0033937515690922737\n",
      "Epoch 206, Train-Loss 0.003808123990893364\n",
      "Epoch 207, Train-Loss 0.0032454945612698793\n",
      "Epoch 208, Train-Loss 0.002854736289009452\n",
      "Epoch 209, Train-Loss 0.0031533942092210054\n",
      "Epoch 210, Train-Loss 0.002855804516002536\n",
      "Epoch 211, Train-Loss 0.0034067188389599323\n",
      "Epoch 212, Train-Loss 0.0031382089946419\n",
      "Epoch 213, Train-Loss 0.0033327583223581314\n",
      "Epoch 214, Train-Loss 0.0032034488394856453\n",
      "Epoch 215, Train-Loss 0.0036702947691082954\n",
      "Epoch 216, Train-Loss 0.003255511401221156\n",
      "Epoch 217, Train-Loss 0.0033966652117669582\n",
      "Epoch 218, Train-Loss 0.0034371742513030767\n",
      "Epoch 219, Train-Loss 0.0031693028286099434\n",
      "Epoch 220, Train-Loss 0.0038070278242230415\n",
      "Epoch 221, Train-Loss 0.00355449877679348\n",
      "Epoch 222, Train-Loss 0.003417965956032276\n",
      "Epoch 223, Train-Loss 0.0035437168553471565\n",
      "Epoch 224, Train-Loss 0.0035312436521053314\n",
      "Epoch 225, Train-Loss 0.0032216007821261883\n",
      "Epoch 226, Train-Loss 0.002882920205593109\n",
      "Epoch 227, Train-Loss 0.002949865534901619\n",
      "Epoch 228, Train-Loss 0.0031080765184015036\n",
      "Epoch 229, Train-Loss 0.003273474983870983\n",
      "Epoch 230, Train-Loss 0.002858878578990698\n",
      "Epoch 231, Train-Loss 0.0035300040617585182\n",
      "Epoch 232, Train-Loss 0.0032374518923461437\n",
      "Epoch 233, Train-Loss 0.0027077780105173588\n",
      "Epoch 234, Train-Loss 0.0038146269507706165\n",
      "Epoch 235, Train-Loss 0.0034965742379426956\n",
      "Epoch 236, Train-Loss 0.0028220494277775288\n",
      "Epoch 237, Train-Loss 0.003228480461984873\n",
      "Epoch 238, Train-Loss 0.003269366454333067\n",
      "Epoch 239, Train-Loss 0.0034252568148076534\n",
      "Epoch 240, Train-Loss 0.003099597990512848\n",
      "Epoch 241, Train-Loss 0.004348520655184984\n",
      "Epoch 242, Train-Loss 0.0035337572917342186\n",
      "Epoch 243, Train-Loss 0.0032136195804923773\n",
      "Epoch 244, Train-Loss 0.0034041577018797398\n",
      "Epoch 245, Train-Loss 0.002847875002771616\n",
      "Epoch 246, Train-Loss 0.003270893357694149\n",
      "Epoch 247, Train-Loss 0.0037323515862226486\n",
      "Epoch 248, Train-Loss 0.002711761277168989\n",
      "Epoch 249, Train-Loss 0.003318328410387039\n",
      "Epoch 250, Train-Loss 0.0028422882314771414\n",
      "Epoch 251, Train-Loss 0.0028284292202442884\n",
      "Epoch 252, Train-Loss 0.0029904881957918406\n",
      "Epoch 253, Train-Loss 0.0026525617577135563\n",
      "Epoch 254, Train-Loss 0.002694130875170231\n",
      "Epoch 255, Train-Loss 0.0021886087488383055\n",
      "Epoch 256, Train-Loss 0.003123121801763773\n",
      "Epoch 257, Train-Loss 0.0028569588903337717\n",
      "Epoch 258, Train-Loss 0.002446977188810706\n",
      "Epoch 259, Train-Loss 0.0028684274293482304\n",
      "Epoch 260, Train-Loss 0.0028176670894026756\n",
      "Epoch 261, Train-Loss 0.0025888578966259956\n",
      "Epoch 262, Train-Loss 0.0026744231581687927\n",
      "Epoch 263, Train-Loss 0.0023913285695016384\n",
      "Epoch 264, Train-Loss 0.0027127210050821304\n",
      "Epoch 265, Train-Loss 0.002942995633929968\n",
      "Epoch 266, Train-Loss 0.0024373242631554604\n",
      "Epoch 267, Train-Loss 0.0027272701263427734\n",
      "Epoch 268, Train-Loss 0.0026212427765130997\n",
      "Epoch 269, Train-Loss 0.0026820972561836243\n",
      "Epoch 270, Train-Loss 0.002416847972199321\n",
      "Epoch 271, Train-Loss 0.002631059382110834\n",
      "Epoch 272, Train-Loss 0.002569016069173813\n",
      "Epoch 273, Train-Loss 0.002413762267678976\n",
      "Epoch 274, Train-Loss 0.0026604013983160257\n",
      "Epoch 275, Train-Loss 0.0027555644046515226\n",
      "Epoch 276, Train-Loss 0.00225282134488225\n",
      "Epoch 277, Train-Loss 0.002473304746672511\n",
      "Epoch 278, Train-Loss 0.0027056513354182243\n",
      "Epoch 279, Train-Loss 0.002890873234719038\n",
      "Epoch 280, Train-Loss 0.0026162457652390003\n",
      "Epoch 281, Train-Loss 0.0023436294868588448\n",
      "Epoch 282, Train-Loss 0.0023778120521456003\n",
      "Epoch 283, Train-Loss 0.0032669459469616413\n",
      "Epoch 284, Train-Loss 0.0025295293889939785\n",
      "Epoch 285, Train-Loss 0.0024663459043949842\n",
      "Epoch 286, Train-Loss 0.0026549797039479017\n",
      "Epoch 287, Train-Loss 0.002451073145493865\n",
      "Epoch 288, Train-Loss 0.002636183984577656\n",
      "Epoch 289, Train-Loss 0.002610814291983843\n",
      "Epoch 290, Train-Loss 0.0025196627248078585\n",
      "Epoch 291, Train-Loss 0.0024978569708764553\n",
      "Epoch 292, Train-Loss 0.0030113079119473696\n",
      "Epoch 293, Train-Loss 0.002677619457244873\n",
      "Epoch 294, Train-Loss 0.0026039450895041227\n",
      "Epoch 295, Train-Loss 0.0025208787992596626\n",
      "Epoch 296, Train-Loss 0.002671512309461832\n",
      "Epoch 297, Train-Loss 0.002756803994998336\n",
      "Epoch 298, Train-Loss 0.002435472793877125\n",
      "Epoch 299, Train-Loss 0.002561246044933796\n",
      "Epoch 300, Train-Loss 0.0022028754465281963\n",
      "Epoch 301, Train-Loss 0.002585155423730612\n",
      "Epoch 302, Train-Loss 0.0024110632948577404\n",
      "Epoch 303, Train-Loss 0.0024873872753232718\n",
      "Epoch 304, Train-Loss 0.002522821072489023\n",
      "Epoch 305, Train-Loss 0.0036279617343097925\n",
      "Epoch 306, Train-Loss 0.0025122719816863537\n",
      "Epoch 307, Train-Loss 0.002336798468604684\n",
      "Epoch 308, Train-Loss 0.0025987583212554455\n",
      "Epoch 309, Train-Loss 0.0026440811343491077\n",
      "Epoch 310, Train-Loss 0.0024464125744998455\n",
      "Epoch 311, Train-Loss 0.0029726019129157066\n",
      "Epoch 312, Train-Loss 0.0025489437393844128\n",
      "Epoch 313, Train-Loss 0.0025885470677167177\n",
      "Epoch 314, Train-Loss 0.002669921610504389\n",
      "Epoch 315, Train-Loss 0.002654168289154768\n",
      "Epoch 316, Train-Loss 0.002469593659043312\n",
      "Epoch 317, Train-Loss 0.002723963465541601\n",
      "Epoch 318, Train-Loss 0.0024387198500335217\n",
      "Epoch 319, Train-Loss 0.002726183272898197\n",
      "Epoch 320, Train-Loss 0.002726783510297537\n",
      "Epoch 321, Train-Loss 0.002249866258352995\n",
      "Epoch 322, Train-Loss 0.0024329018779098988\n",
      "Epoch 323, Train-Loss 0.0026969527825713158\n",
      "Epoch 324, Train-Loss 0.0020218610297888517\n",
      "Epoch 325, Train-Loss 0.002521389164030552\n",
      "Epoch 326, Train-Loss 0.002701300662010908\n",
      "Epoch 327, Train-Loss 0.0022284123115241528\n",
      "Epoch 328, Train-Loss 0.0024432619102299213\n",
      "Epoch 329, Train-Loss 0.002720535034313798\n",
      "Epoch 330, Train-Loss 0.002244147239252925\n",
      "Epoch 331, Train-Loss 0.0023486162535846233\n",
      "Epoch 332, Train-Loss 0.002700886921957135\n",
      "Epoch 333, Train-Loss 0.0021525798365473747\n",
      "Epoch 334, Train-Loss 0.002407953143119812\n",
      "Epoch 335, Train-Loss 0.0024075526744127274\n",
      "Epoch 336, Train-Loss 0.0023658303543925285\n",
      "Epoch 337, Train-Loss 0.0026148660108447075\n",
      "Epoch 338, Train-Loss 0.002251522382721305\n",
      "Epoch 339, Train-Loss 0.002960538724437356\n",
      "Epoch 340, Train-Loss 0.002664940431714058\n",
      "Epoch 341, Train-Loss 0.002191411331295967\n",
      "Epoch 342, Train-Loss 0.0020370124839246273\n",
      "Epoch 343, Train-Loss 0.0021546564530581236\n",
      "Epoch 344, Train-Loss 0.0023592275101691484\n",
      "Epoch 345, Train-Loss 0.0025274946819990873\n",
      "Epoch 346, Train-Loss 0.00262545351870358\n",
      "Epoch 347, Train-Loss 0.0022830157540738583\n",
      "Epoch 348, Train-Loss 0.0024524249602109194\n",
      "Epoch 349, Train-Loss 0.002748038386926055\n",
      "Epoch 350, Train-Loss 0.00585206039249897\n",
      "Epoch 351, Train-Loss 0.002111435867846012\n",
      "Epoch 352, Train-Loss 0.002319047227501869\n",
      "Epoch 353, Train-Loss 0.002481417264789343\n",
      "Epoch 354, Train-Loss 0.0021862939465790987\n",
      "Epoch 355, Train-Loss 0.0024863656144589186\n",
      "Epoch 356, Train-Loss 0.0024165885988622904\n",
      "Epoch 357, Train-Loss 0.002480260794982314\n",
      "Epoch 358, Train-Loss 0.002715806243941188\n",
      "Epoch 359, Train-Loss 0.003106391988694668\n",
      "Epoch 360, Train-Loss 0.0026436145417392254\n",
      "Epoch 361, Train-Loss 0.0023751556873321533\n",
      "Epoch 362, Train-Loss 0.002552999649196863\n",
      "Epoch 363, Train-Loss 0.002301741624251008\n",
      "Epoch 364, Train-Loss 0.001992085948586464\n",
      "Epoch 365, Train-Loss 0.002997711766511202\n",
      "Epoch 366, Train-Loss 0.0024514307733625174\n",
      "Epoch 367, Train-Loss 0.002056804019957781\n",
      "Epoch 368, Train-Loss 0.002567784395068884\n",
      "Epoch 369, Train-Loss 0.0023788977414369583\n",
      "Epoch 370, Train-Loss 0.0027014920487999916\n",
      "Epoch 371, Train-Loss 0.002393502974882722\n",
      "Epoch 372, Train-Loss 0.002362294355407357\n",
      "Epoch 373, Train-Loss 0.002395557938143611\n",
      "Epoch 374, Train-Loss 0.0022890963591635227\n",
      "Epoch 375, Train-Loss 0.0028571784496307373\n",
      "Epoch 376, Train-Loss 0.0023281986359506845\n",
      "Epoch 377, Train-Loss 0.0021457138936966658\n",
      "Epoch 378, Train-Loss 0.0024876848328858614\n",
      "Epoch 379, Train-Loss 0.002555037382990122\n",
      "Epoch 380, Train-Loss 0.0023669530637562275\n",
      "Epoch 381, Train-Loss 0.002242209855467081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 382, Train-Loss 0.0025423411279916763\n",
      "Epoch 383, Train-Loss 0.0026308693923056126\n",
      "Epoch 384, Train-Loss 0.0022720899432897568\n",
      "Epoch 385, Train-Loss 0.002573352074250579\n",
      "Epoch 386, Train-Loss 0.0022517754696309566\n",
      "Epoch 387, Train-Loss 0.0023414178285747766\n",
      "Epoch 388, Train-Loss 0.002599644009023905\n",
      "Epoch 389, Train-Loss 0.002408759668469429\n",
      "Epoch 390, Train-Loss 0.0023620440624654293\n",
      "Epoch 391, Train-Loss 0.0024382686242461205\n",
      "Epoch 392, Train-Loss 0.00253239250741899\n",
      "Epoch 393, Train-Loss 0.0026353541761636734\n",
      "Epoch 394, Train-Loss 0.002531664911657572\n",
      "Epoch 395, Train-Loss 0.002752631902694702\n",
      "Epoch 396, Train-Loss 0.0022269897162914276\n",
      "Epoch 397, Train-Loss 0.002100476296618581\n",
      "Epoch 398, Train-Loss 0.002462774980813265\n",
      "Epoch 399, Train-Loss 0.002560768276453018\n",
      "Epoch 400, Train-Loss 0.0026526423171162605\n",
      "Epoch 401, Train-Loss 0.002411983907222748\n",
      "Epoch 402, Train-Loss 0.0027821867261081934\n",
      "Epoch 403, Train-Loss 0.002499461406841874\n",
      "Epoch 404, Train-Loss 0.002489402424544096\n",
      "Epoch 405, Train-Loss 0.0024032508954405785\n",
      "Epoch 406, Train-Loss 0.0024570932146161795\n",
      "Epoch 407, Train-Loss 0.0028484459035098553\n",
      "Epoch 408, Train-Loss 0.0025014423299580812\n",
      "Epoch 409, Train-Loss 0.003110934980213642\n",
      "Epoch 410, Train-Loss 0.0024300585500895977\n",
      "Epoch 411, Train-Loss 0.0024087433703243732\n",
      "Epoch 412, Train-Loss 0.0022533335722982883\n",
      "Epoch 413, Train-Loss 0.0024621691554784775\n",
      "Epoch 414, Train-Loss 0.002388695254921913\n",
      "Epoch 415, Train-Loss 0.0022633401677012444\n",
      "Epoch 416, Train-Loss 0.0025280811823904514\n",
      "Epoch 417, Train-Loss 0.0023267162032425404\n",
      "Epoch 418, Train-Loss 0.0026395688764750957\n",
      "Epoch 419, Train-Loss 0.0026535559445619583\n",
      "Epoch 420, Train-Loss 0.0025883540511131287\n",
      "Epoch 421, Train-Loss 0.0025527148973196745\n",
      "Epoch 422, Train-Loss 0.002837452106177807\n",
      "Epoch 423, Train-Loss 0.001965397037565708\n",
      "Epoch 424, Train-Loss 0.0023367712274193764\n",
      "Epoch 425, Train-Loss 0.0018397208768874407\n",
      "Epoch 426, Train-Loss 0.0027266195975244045\n",
      "Epoch 427, Train-Loss 0.0023659435100853443\n",
      "Epoch 428, Train-Loss 0.002628467744216323\n",
      "Epoch 429, Train-Loss 0.002672904636710882\n",
      "Epoch 430, Train-Loss 0.0023714392445981503\n",
      "Epoch 431, Train-Loss 0.0024687140248715878\n",
      "Epoch 432, Train-Loss 0.0021285631228238344\n",
      "Epoch 433, Train-Loss 0.0022998619824647903\n",
      "Epoch 434, Train-Loss 0.002367432927712798\n",
      "Epoch 435, Train-Loss 0.0024476738180965185\n",
      "Epoch 436, Train-Loss 0.0024794586934149265\n",
      "Epoch 437, Train-Loss 0.0025719967670738697\n",
      "Epoch 438, Train-Loss 0.0024006820749491453\n",
      "Epoch 439, Train-Loss 0.002192495856434107\n",
      "Epoch 440, Train-Loss 0.0023266817443072796\n",
      "Epoch 441, Train-Loss 0.0026780031621456146\n",
      "Epoch 442, Train-Loss 0.0023652869276702404\n",
      "Epoch 443, Train-Loss 0.0022328351624310017\n",
      "Epoch 444, Train-Loss 0.0025271140038967133\n",
      "Epoch 445, Train-Loss 0.002043255139142275\n",
      "Epoch 446, Train-Loss 0.0024516915436834097\n",
      "Epoch 447, Train-Loss 0.002117720665410161\n",
      "Epoch 448, Train-Loss 0.002267114818096161\n",
      "Epoch 449, Train-Loss 0.0020706304349005222\n",
      "Epoch 450, Train-Loss 0.002602478489279747\n",
      "Epoch 451, Train-Loss 0.002034137723967433\n",
      "Epoch 452, Train-Loss 0.0026341350749135017\n",
      "Epoch 453, Train-Loss 0.002577992156147957\n",
      "Epoch 454, Train-Loss 0.0027930079959332943\n",
      "Epoch 455, Train-Loss 0.0025663634296506643\n",
      "Epoch 456, Train-Loss 0.0022249026224017143\n",
      "Epoch 457, Train-Loss 0.0024249423295259476\n",
      "Epoch 458, Train-Loss 0.002794839907437563\n",
      "Epoch 459, Train-Loss 0.002259264700114727\n",
      "Epoch 460, Train-Loss 0.002449802588671446\n",
      "Epoch 461, Train-Loss 0.002674118848517537\n",
      "Epoch 462, Train-Loss 0.0025386083871126175\n",
      "Epoch 463, Train-Loss 0.0023265364579856396\n",
      "Epoch 464, Train-Loss 0.0022725057788193226\n",
      "Epoch 465, Train-Loss 0.0023552817292511463\n",
      "Epoch 466, Train-Loss 0.002468331716954708\n",
      "Epoch 467, Train-Loss 0.0024464905727654696\n",
      "Epoch 468, Train-Loss 0.0021885698661208153\n",
      "Epoch 469, Train-Loss 0.0022867568768560886\n",
      "Epoch 470, Train-Loss 0.0025703744031488895\n",
      "Epoch 471, Train-Loss 0.002985481172800064\n",
      "Epoch 472, Train-Loss 0.002380669815465808\n",
      "Epoch 473, Train-Loss 0.0020035819616168737\n",
      "Epoch 474, Train-Loss 0.002289926866069436\n",
      "Epoch 475, Train-Loss 0.0024758223444223404\n",
      "Epoch 476, Train-Loss 0.001958065666258335\n",
      "Epoch 477, Train-Loss 0.0027707000263035297\n",
      "Epoch 478, Train-Loss 0.0023020319640636444\n",
      "Epoch 479, Train-Loss 0.0025962195359170437\n",
      "Epoch 480, Train-Loss 0.002308856463059783\n",
      "Epoch 481, Train-Loss 0.0023273350670933723\n",
      "Epoch 482, Train-Loss 0.002224852330982685\n",
      "Epoch 483, Train-Loss 0.00271425349637866\n",
      "Epoch 484, Train-Loss 0.002493265550583601\n",
      "Epoch 485, Train-Loss 0.002608239185065031\n",
      "Epoch 486, Train-Loss 0.0020637151319533587\n",
      "Epoch 487, Train-Loss 0.0022682317066937685\n",
      "Epoch 488, Train-Loss 0.0018688233103603125\n",
      "Epoch 489, Train-Loss 0.002686725929379463\n",
      "Epoch 490, Train-Loss 0.002806867938488722\n",
      "Epoch 491, Train-Loss 0.002633592812344432\n",
      "Epoch 492, Train-Loss 0.002430229913443327\n",
      "Epoch 493, Train-Loss 0.002341327490285039\n",
      "Epoch 494, Train-Loss 0.002609656658023596\n",
      "Epoch 495, Train-Loss 0.002640672028064728\n",
      "Epoch 496, Train-Loss 0.0022058519534766674\n",
      "Epoch 497, Train-Loss 0.0022039846517145634\n",
      "Epoch 498, Train-Loss 0.0027338883373886347\n",
      "Epoch 499, Train-Loss 0.0022271517664194107\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dset = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), y_train.astype(np.float32)))\n",
    "train_dset = train_dset.shuffle(buffer_size=x_train.shape[0]+256).batch(64)\n",
    "test_losses = []\n",
    "\n",
    "optimizer = tf.optimizers.Adam(1e-3)\n",
    "model = Temporal_Specgram_CNN_Model(optimizer=optimizer)\n",
    "x_val, y_val = x_val.astype(np.float32), y_val.astype(np.float32)\n",
    "with tf.device('/device:gpu:0'):\n",
    "#     tf.print('Training Fold {}'.format(index))\n",
    "    #model.load_weights('./temporal_specgram_weights/initial')\n",
    "    for epoch in range(500):\n",
    "        if epoch>250:\n",
    "            model.optimizer.learning_rate=2e-4\n",
    "        for step, train_batch in enumerate(train_dset):\n",
    "            train_loss = model.train_model(train_batch[0], train_batch[1])\n",
    "\n",
    "        tf.print('Epoch {}, Train-Loss {}'.format(epoch, train_loss), output_stream=sys.stdout)\n",
    "        test_losses.append(model.compute_test_loss(x_val, y_val))\n",
    "        \n",
    "    plt.plot(test_losses)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Test Loss')\n",
    "    plt.title('Test Loss by Epoch: {} units'.format(256))\n",
    "    #plt.savefig('mnt/cube/srrudrar/temporal_model/loss_plots/{}units_loss.png'.format(256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[21357,64,30,30] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b2114d02953f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0menc_train1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0menc_train2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0menc_train3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#enc_train4 = model.enc_2(model.enc_dropout(model.enc_1(x_train[45000:60000,:,:][:,:,:,np.newaxis])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#enc_train5 = model.enc_2(model.enc_dropout(model.enc_1(x_train[60000:80000,:,:][:,:,:,np.newaxis])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \"\"\"\n\u001b[1;32m    424\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 425\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1018\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1148\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2602\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2605\u001b[0m   return squeeze_batch_dims(\n\u001b[1;32m   2606\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    930\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[21357,64,30,30] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:gpu:3'):\n",
    "    #model.save_weights('/mnt/cube/srrudrar/temporal_model/temporal_model_weights/temp_256_l1_3_5_500epoch_64batch.h5')\n",
    "    #predicted = model.full_model(x_val)\n",
    "    enc_train1 = model.enc_2(model.enc_dropout(model.enc_1(x_train[:15000,:,:][:,:,:,np.newaxis])))\n",
    "    enc_train2 = model.enc_2(model.enc_dropout(model.enc_1(x_train[15000:30000,:,:][:,:,:,np.newaxis])))\n",
    "    enc_train3 = model.enc_2(model.enc_dropout(model.enc_1(x_train[30000:,:,:][:,:,:,np.newaxis])))\n",
    "    #enc_train4 = model.enc_2(model.enc_dropout(model.enc_1(x_train[45000:60000,:,:][:,:,:,np.newaxis])))\n",
    "    #enc_train5 = model.enc_2(model.enc_dropout(model.enc_1(x_train[60000:80000,:,:][:,:,:,np.newaxis])))\n",
    "    #enc_train6 = model.enc_2(model.enc_dropout(model.enc_1(x_train[80000:,:,:][:,:,:,np.newaxis])))\n",
    "    enc_val = model.enc_2(model.enc_dropout(model.enc_1(x_val[:,:,:,np.newaxis])))\n",
    "    #tf.print(predicted.shape)\n",
    "    tf.print(enc_train1.shape)\n",
    "    tf.print(enc_train2.shape)\n",
    "    tf.print(enc_train3.shape)\n",
    "    tf.print(enc_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.vstack([model.dec_pred(enc_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.dec_pred(enc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = np.vstack([enc_train1, enc_train2, enc_train3])\n",
    "enc_val = np.vstack([enc_val])\n",
    "\n",
    "enc_train_reshape = np.reshape(enc_train, (len(enc_train),8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51357, 32), (51357, 32, 32), (5706, 32), (5706, 32, 32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(enc_train), np.shape(x_train), np.shape(enc_val), np.shape(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5706, 32), (5706, 32))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(pred_test), np.shape(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.05, 'Mel Spectrogram')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(4,5))\n",
    "cax = ax.matshow(x_train[100], interpolation='nearest', aspect='auto', cmap=plt.cm.afmhot, origin='lower')\n",
    "fig.colorbar(cax)\n",
    "plt.title('Mel Spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.05, 'Mel Spectrogram')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(4,5))\n",
    "cax = ax.matshow(enc_train_reshape[100], interpolation='nearest', aspect='auto', cmap=plt.cm.afmhot, origin='lower')\n",
    "fig.colorbar(cax)\n",
    "plt.title('Mel Spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_train = x_train1[:, 32:, :]\n",
    "spec_train = [x.flatten() for x in x_train]\n",
    "#enc_train\n",
    "\n",
    "resp_test = x_val1[:, 32:, :]\n",
    "spec_test = [x.flatten() for x in x_val]\n",
    "#enc_test = enc_val\n",
    "#x_val = x_val[:, :16, :]\n",
    "#y_train = y_train[:, :16]\n",
    "#y_val = y_val[:, :16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51357, 1024),\n",
       " (51357, 32),\n",
       " (51357, 214, 32),\n",
       " (5706, 1024),\n",
       " (5706, 32),\n",
       " (5706, 214, 32))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(spec_train), np.shape(enc_train), np.shape(resp_train), np.shape(spec_test), np.shape(enc_val), np.shape(resp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/mnt/cube/srrudrar/tcm_model/{}/32x32/stim_resp_32x32_{}/spec_train_{}.npy'.format(bird,d,d), spec_train)\n",
    "np.save('/mnt/cube/srrudrar/tcm_model/{}/32x32/stim_resp_32x32_{}/enc_train_{}.npy'.format(bird,d,d), enc_train)\n",
    "np.save('/mnt/cube/srrudrar/tcm_model/{}/32x32/stim_resp_32x32_{}/resp_train_{}.npy'.format(bird,d,d), resp_train)\n",
    "\n",
    "np.save('/mnt/cube/srrudrar/tcm_model/{}/32x32/stim_resp_32x32_{}/spec_test_{}.npy'.format(bird,d,d), spec_test)\n",
    "np.save('/mnt/cube/srrudrar/tcm_model/{}/32x32/stim_resp_32x32_{}/enc_test_{}.npy'.format(bird,d,d), enc_val)\n",
    "np.save('/mnt/cube/srrudrar/tcm_model/{}/32x32/stim_resp_32x32_{}/resp_test_{}.npy'.format(bird,d,d), resp_test)\n",
    "        \n",
    "np.save('/mnt/cube/srrudrar/tcm_model/{}/32x32/stim_resp_32x32_{}/y_train_{}.npy'.format(bird,d,d), y_train)\n",
    "np.save('/mnt/cube/srrudrar/tcm_model/{}/32x32/stim_resp_32x32_{}/y_test_{}.npy'.format(bird,d,d), y_val)\n",
    "\n",
    "np.save('/mnt/cube/srrudrar/tcm_model/{}/32x32/stim_resp_32x32_{}/pred_test_{}.npy'.format(bird,d,d), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump([float(i[0].numpy()) for i in test_losses],open('pred_only_64_channels_dropout_test_loss.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
