{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "gpus = [0]\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=','.join([str(i) for i in gpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow.keras as keras\n",
    "import numpy as np \n",
    "#from oe_acute import MNE\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "#from AE import MDSAE as ae\n",
    "#from network_visualisation import plot_these_aud_weights\n",
    "#import network_visualisation\n",
    "#import quantify_aud_strfs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird = 'B1692'\n",
    "d = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocate GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = [0] # Here I set CUDA to only see one GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=','.join([str(i) for i in gpus])\n",
    "num_gpus = len(gpus) # number of GPUs to use\n",
    "if len(gpus) < 1:\n",
    "    num_gpus = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "print( [x.name for x in local_device_protos if x.device_type == 'GPU'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_Specgram_CNN_Model(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Temporal_Specgram_CNN_Model, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.enc_1=keras.Sequential([\n",
    "            keras.Input(shape=(32, 32, 1)),\n",
    "#             keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\",),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size=3, strides=(1, 1), activation=\"relu\",),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            \n",
    "            \n",
    "        ])\n",
    "        self.enc_dropout=tf.keras.layers.Dropout(0.5)\n",
    "        self.enc_2=tf.keras.layers.Dense(units=d,activation='sigmoid', kernel_regularizer=keras.regularizers.L1(10**-3.5),)\n",
    "        self.dec_recon=keras.Sequential([\n",
    "            tf.keras.layers.Dense(units=16 * 16 * 256, activation=\"relu\", kernel_regularizer=keras.regularizers.L1(10**-3.5)),\n",
    "            tf.keras.layers.Reshape(target_shape=(16, 16, 256)),\n",
    "#             tf.keras.layers.Conv2DTfranspose(\n",
    "#                 filters=16, kernel_size=2, strides=(2, 2),  activation=\"relu\", kernel_regularizer=keras.regularizers.L1(10**-3.5), \n",
    "#             ),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=8, kernel_size=2, strides=(2, 2), activation=\"relu\", kernel_regularizer=keras.regularizers.L1(10**-3.5)\n",
    "            ),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=1, kernel_size=1, strides=(1, 1), kernel_regularizer=keras.regularizers.L1(10**-3.5)\n",
    "            ),\n",
    "        ])\n",
    "        self.dec_pred=keras.Sequential([tf.keras.layers.Dense(units=32, kernel_regularizer=keras.regularizers.L1(10**-3.5))])\n",
    "        self.recon_losses=[]\n",
    "        self.pred_losses=[]\n",
    "    @tf.function\n",
    "    def get_loss(self, x_t, y_t):\n",
    "        #print(x_t)#.shape\n",
    "        x_hat, y_hat = self(tf.expand_dims(x_t, -1))\n",
    "        pred_losses=tf.reduce_mean(tf.square(y_t - y_hat))\n",
    "        \n",
    "        recon_losses=tf.reduce_mean(tf.square(x_t - tf.squeeze(x_hat, -1)))\n",
    "\n",
    "        \n",
    "        return pred_losses,recon_losses\n",
    "\n",
    "    @tf.function\n",
    "    def get_gradients(self, x_t, y_t):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_losses,recon_losses = self.get_loss(x_t, y_t)\n",
    "            #tf.print(pred_losses, recon_losses)\n",
    "            loss=pred_losses+0.5*recon_losses\n",
    "            \n",
    "        return loss, tape.gradient(loss, self.enc_1.trainable_variables+self.enc_2.trainable_variables+self.dec_recon.trainable_variables+self.dec_pred.trainable_variables)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_model(self, X_train, y_train):\n",
    "        loss, gradients = self.get_gradients(X_train, y_train)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.enc_1.trainable_variables+self.enc_2.trainable_variables+self.dec_recon.trainable_variables+self.dec_pred.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def compute_test_loss(self, X_test, y_test):\n",
    "        x_hat, y_hat = self.predict(tf.expand_dims(X_test, -1))\n",
    "        pred_losses=tf.reduce_mean(tf.square(y_test - y_hat))\n",
    "        \n",
    "        recon_losses=tf.reduce_mean(tf.square(X_test - tf.squeeze(x_hat, -1)))\n",
    "\n",
    "        \n",
    "        return pred_losses,recon_losses\n",
    "\n",
    "\n",
    "    def call(self, input):\n",
    "        latent=self.enc_1(input)\n",
    "        latent=self.enc_dropout(latent, training=True)\n",
    "        latent=self.enc_2(latent)\n",
    "        return self.dec_recon(latent), self.dec_pred(latent)\n",
    "\n",
    "    def predict(self, input):\n",
    "        latent=self.enc_1(input)\n",
    "        latent=self.enc_dropout(latent, training=False)\n",
    "        latent=self.enc_2(latent)\n",
    "        return self.dec_recon(latent), self.dec_pred(latent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.enc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_t, y_t = train_batch[0], train_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_losses,recon_losses=model.get_loss(x_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def extract_spec_data(x_path, y_path, random_seed=None, global_normalize=False):\n",
    "def extract_spec_data(x_path1, x_path2, x_path3, x_path4, x_path5, y_path, random_seed=None):\n",
    "    #all_curr=np.load(x_path,allow_pickle=True)\n",
    "    #all_next=np.load(y_path,allow_pickle=True)\n",
    "    all_curr1 = pickle.load(open(x_path1, 'rb'))\n",
    "    all_curr2 = pickle.load(open(x_path2, 'rb'))\n",
    "    all_curr3 = pickle.load(open(x_path3, 'rb'))\n",
    "    all_curr4 = pickle.load(open(x_path4, 'rb'))\n",
    "    all_curr5 = pickle.load(open(x_path5, 'rb'))\n",
    "\n",
    "    all_next=np.load(y_path,allow_pickle=True)\n",
    "    \n",
    "    #x_array = all_curr\n",
    "    x_array = np.concatenate((all_curr1, all_curr2, all_curr3, all_curr4, all_curr5), axis=0)\n",
    "    y_array = all_next\n",
    "\n",
    "    #x_array = np.vstack(all_curr)\n",
    "    #y_array = np.vstack(all_next)\n",
    "\n",
    "    if random_seed is None:\n",
    "        rand_idx=np.arange(0, np.shape(x_array)[0])\n",
    "    else:\n",
    "        np.random.seed(random_seed)\n",
    "        rand_idx=np.random.choice(range(np.shape(x_array)[0]), size=np.shape(x_array)[0],replace=False)\n",
    "    \n",
    "    split_train_idx, split_val_idx = rand_idx[np.shape(x_array)[0]//10:],rand_idx[:np.shape(x_array)[0]//10] \n",
    "    x_train, x_val=np.asarray(x_array)[split_train_idx], np.asarray(x_array)[split_val_idx]\n",
    "    y_train, y_val=np.asarray(y_array)[split_train_idx],np.asarray(y_array)[split_val_idx]\n",
    "    \n",
    "    #if global_normalize:\n",
    "    #    x_train=x_train/x_train.max()\n",
    "    #    y_train=y_train/y_train.max()\n",
    "    #    x_val=x_val/x_val.max()\n",
    "    #    y_val=y_val/y_val.max()\n",
    "    return x_train,y_train,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs_path1 = '/mnt/cube/srrudrar/stim_preprocess/temporal_model/{}/segs_list_{}_32_part1.pkl'.format(bird, bird)     ###change\n",
    "segs_path2 = '/mnt/cube/srrudrar/stim_preprocess/temporal_model/{}/segs_list_{}_32_part2.pkl'.format(bird, bird)     ###change\n",
    "segs_path3 = '/mnt/cube/srrudrar/stim_preprocess/temporal_model/{}/segs_list_{}_32_part3.pkl'.format(bird, bird)     ###change\n",
    "segs_path4 = '/mnt/cube/srrudrar/stim_preprocess/temporal_model/{}/segs_list_{}_32_part4.pkl'.format(bird, bird)     ###change\n",
    "segs_path5 = '/mnt/cube/srrudrar/stim_preprocess/temporal_model/{}/segs_list_{}_32_part5.pkl'.format(bird, bird)     ###change\n",
    "next_path = '/mnt/cube/srrudrar/stim_preprocess/temporal_model/{}/next_list_{}_32.pkl'.format(bird, bird)            ###change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train,y_train,x_val,y_val = extract_spec_data(segs_path, next_path, random_seed=0,global_normalize=True)\n",
    "x_train1,y_train1,x_val1,y_val1 = extract_spec_data(segs_path1,segs_path2,segs_path3,segs_path4,segs_path5,next_path,random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train1[:, :32, :]\n",
    "x_val = x_val1[:, :32, :]\n",
    "y_train = y_train1[:, :32]\n",
    "y_val = y_val1[:, :32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((243675, 32, 32), (27074, 32, 32), (243675, 32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train), np.shape(x_val), np.shape(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), y_train.astype(np.float32)))\n",
    "train_dset = train_dset.shuffle(buffer_size=x_train.shape[0]+256).batch(64)\n",
    "test_losses = []\n",
    "\n",
    "optimizer = tf.optimizers.Adam(1e-3)\n",
    "model = Temporal_Specgram_CNN_Model(optimizer=optimizer)\n",
    "x_val, y_val = x_val.astype(np.float32), y_val.astype(np.float32)\n",
    "with tf.device('/device:gpu:0'):\n",
    "#     tf.print('Training Fold {}'.format(index))\n",
    "    #model.load_weights('./temporal_specgram_weights/initial')\n",
    "    for epoch in range(500):\n",
    "        if epoch>250:\n",
    "            model.optimizer.learning_rate=2e-4\n",
    "        for step, train_batch in enumerate(train_dset):\n",
    "            train_loss = model.train_model(train_batch[0], train_batch[1])\n",
    "\n",
    "        tf.print('Epoch {}, Train-Loss {}'.format(epoch, train_loss), output_stream=sys.stdout)\n",
    "        test_losses.append(model.compute_test_loss(x_val, y_val))\n",
    "        \n",
    "    plt.plot(test_losses)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Test Loss')\n",
    "    plt.title('Test Loss by Epoch: {} units'.format(256))\n",
    "    #plt.savefig('mnt/cube/srrudrar/temporal_model/loss_plots/{}units_loss.png'.format(256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:gpu:3'):\n",
    "    #model.save_weights('/mnt/cube/srrudrar/temporal_model/temporal_model_weights/temp_256_l1_3_5_500epoch_64batch.h5')\n",
    "    #predicted = model.full_model(x_val)\n",
    "    enc_train1 = model.enc_2(model.enc_dropout(model.enc_1(x_train[:15000,:,:][:,:,:,np.newaxis])))\n",
    "    enc_train2 = model.enc_2(model.enc_dropout(model.enc_1(x_train[15000:30000,:,:][:,:,:,np.newaxis])))\n",
    "    enc_train3 = model.enc_2(model.enc_dropout(model.enc_1(x_train[30000:45000,:,:][:,:,:,np.newaxis])))\n",
    "    enc_train4 = model.enc_2(model.enc_dropout(model.enc_1(x_train[45000:60000,:,:][:,:,:,np.newaxis])))\n",
    "    enc_train5 = model.enc_2(model.enc_dropout(model.enc_1(x_train[60000:80000,:,:][:,:,:,np.newaxis])))\n",
    "    enc_train6 = model.enc_2(model.enc_dropout(model.enc_1(x_train[80000:,:,:][:,:,:,np.newaxis])))\n",
    "    enc_val = model.enc_2(model.enc_dropout(model.enc_1(x_val[:,:,:,np.newaxis])))\n",
    "    #tf.print(predicted.shape)\n",
    "    tf.print(enc_train1.shape)\n",
    "    tf.print(enc_train2.shape)\n",
    "    tf.print(enc_train3.shape)\n",
    "    tf.print(enc_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = np.vstack([enc_train1, enc_train2, enc_train3, enc_train4, enc_train5, enc_train6])\n",
    "enc_val = np.vstack([enc_val])\n",
    "\n",
    "enc_train_reshape = np.reshape(enc_train, (len(enc_train),32,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(enc_train), np.shape(x_train), np.shape(enc_val), np.shape(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(4,5))\n",
    "cax = ax.matshow(x_train[100], interpolation='nearest', aspect='auto', cmap=plt.cm.afmhot, origin='lower')\n",
    "fig.colorbar(cax)\n",
    "plt.title('Mel Spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(4,5))\n",
    "cax = ax.matshow(enc_train_reshape[100], interpolation='nearest', aspect='auto', cmap=plt.cm.afmhot, origin='lower')\n",
    "fig.colorbar(cax)\n",
    "plt.title('Mel Spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_train = x_train1[:, 32:, :]\n",
    "spec_train = [x.flatten() for x in x_train]\n",
    "#enc_train\n",
    "\n",
    "resp_test = x_val1[:, 32:, :]\n",
    "spec_test = [x.flatten() for x in x_val]\n",
    "#enc_test = enc_val\n",
    "#x_val = x_val[:, :16, :]\n",
    "#y_train = y_train[:, :16]\n",
    "#y_val = y_val[:, :16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(spec_train), np.shape(enc_train), np.shape(resp_train), np.shape(spec_test), np.shape(enc_val), np.shape(resp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 512\n",
    "bird = 'B1596'\n",
    "np.save('/mnt/cube/srrudrar/temporal_cnn_model/{}/32x32/stim_resp_32x32_{}/spec_train_{}.npy'.format(bird,d,d), spec_train)\n",
    "np.save('/mnt/cube/srrudrar/temporal_cnn_model/{}/32x32/stim_resp_32x32_{}/enc_train_{}.npy'.format(bird,d,d), enc_train)\n",
    "np.save('/mnt/cube/srrudrar/temporal_cnn_model/{}/32x32/stim_resp_32x32_{}/resp_train_{}.npy'.format(bird,d,d), resp_train)\n",
    "\n",
    "np.save('/mnt/cube/srrudrar/temporal_cnn_model/{}/32x32/stim_resp_32x32_{}/spec_test_{}.npy'.format(bird,d,d), spec_test)\n",
    "np.save('/mnt/cube/srrudrar/temporal_cnn_model/{}/32x32/stim_resp_32x32_{}/enc_test_{}.npy'.format(bird,d,d), enc_val)\n",
    "np.save('/mnt/cube/srrudrar/temporal_cnn_model/{}/32x32/stim_resp_32x32_{}/resp_test_{}.npy'.format(bird,d,d), resp_test)\n",
    "        \n",
    "np.save('/mnt/cube/srrudrar/temporal_cnn_model/{}/32x32/stim_resp_32x32_{}/y_train_{}.npy'.format(bird,d,d), y_train)\n",
    "np.save('/mnt/cube/srrudrar/temporal_cnn_model/{}/32x32/stim_resp_32x32_{}/y_test_{}.npy'.format(bird,d,d), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump([float(i[0].numpy()) for i in test_losses],open('pred_only_64_channels_dropout_test_loss.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
